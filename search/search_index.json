{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Dream networking and automation","text":"Dream Networking &amp; Automation <p>Transforming business connectivity through smart automation and seamless networking solutions. We make complex networks simple, efficient, and future-ready.</p>"},{"location":"#what-we-do","title":"What We Do","text":"\ud83d\ude80 Network Automation <p>We design and implement robust network automation solutions that streamline your operations, reduce manual errors, and enhance efficiency across your entire infrastructure.</p> \ud83d\udd27 Infrastructure Design <p>Building reliable, scalable infrastructure that keeps your business running smoothly while supporting growth and innovation in an ever-changing digital landscape.</p> \ud83d\udcda Knowledge Transfer <p>We don't just implement solutions \u2013 we empower your team with the knowledge and skills needed to maintain and evolve your network infrastructure independently.</p>"},{"location":"#our-approach","title":"Our ApproachPublic Speaking &amp; Knowledge SharingReady to Transform Your Network?","text":"<p>Dream networking and automation is your trusted partner in transforming networking and automation challenges into strategic advantages. With years of experience in designing and implementing robust network automation solutions, we bring expertise, reliability, and a forward-thinking approach to every project.</p> <p>But it's not just about technical excellence \u2013 it's also about sharing knowledge, empowering your team, and fostering growth within your company. We take the time to understand your unique needs, ensuring that our solutions not only enhance efficiency and security but also strengthen your team's capabilities.</p> <p>By partnering with us, we are committed to delivering customized solutions that support both immediate success and long-term innovation.</p> <p></p> <p>Sharing expertise and insights with the broader community through conference presentations and technical talks.</p> BMP - Going beyond show commands and screen scraping Date: May 30, 2025 |       Event:Autocon3 |       Video: Watch Now <p>Exploring the journey from screen scraping to modern BGP Monitoring Protocol (BMP) solutions. Learn about chosen architecture, challenges with BGP data volume, and practical implementation strategies for handling pre-policy BGP data.</p> Repos are like children, parenting 101 Date: May 16, 2025 |       Event: PyGrunn |       Video: Watch Now <p>A unique perspective on project management through the lens of parenting. Discover how managing code repositories parallels raising children, from creation through maturity, with practical insights on nurturing healthy development practices.</p> <p>Let's discuss how we can help streamline your networking and automation challenges.</p> Get In Touch"},{"location":"blog/","title":"\ud83d\udcdd Blog","text":""},{"location":"blog/tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"blog/tags/#tag:ansible","title":"ansible","text":"<ul> <li>            Managing dockers using ansible on Synology NAS          </li> </ul>"},{"location":"blog/tags/#tag:data-validation","title":"data validation","text":"<ul> <li>            Pydantic series          </li> </ul>"},{"location":"blog/tags/#tag:docker","title":"docker","text":"<ul> <li>            Taskfile magic with docker[-]compose          </li> </ul>"},{"location":"blog/tags/#tag:docker-compose","title":"docker-compose","text":"<ul> <li>            Managing dockers using ansible on Synology NAS          </li> <li>            Taskfile magic with docker[-]compose          </li> </ul>"},{"location":"blog/tags/#tag:email","title":"email","text":"<ul> <li>            Paperless          </li> </ul>"},{"location":"blog/tags/#tag:github-actions","title":"github-actions","text":"<ul> <li>            Pypi Trusted Publisher Management and pitfalls          </li> </ul>"},{"location":"blog/tags/#tag:oidc","title":"oidc","text":"<ul> <li>            Pypi Trusted Publisher Management and pitfalls          </li> </ul>"},{"location":"blog/tags/#tag:paperless","title":"paperless","text":"<ul> <li>            Paperless          </li> </ul>"},{"location":"blog/tags/#tag:poetry","title":"poetry","text":"<ul> <li>            Convert poetry to uv          </li> </ul>"},{"location":"blog/tags/#tag:pydantic","title":"pydantic","text":"<ul> <li>            Pydantic series          </li> </ul>"},{"location":"blog/tags/#tag:pypi","title":"pypi","text":"<ul> <li>            Pypi Trusted Publisher Management and pitfalls          </li> </ul>"},{"location":"blog/tags/#tag:python","title":"python","text":"<ul> <li>            Convert poetry to uv          </li> <li>            Improved shebang for your python standalone script          </li> <li>            Pypi Trusted Publisher Management and pitfalls          </li> </ul>"},{"location":"blog/tags/#tag:script","title":"script","text":"<ul> <li>            Improved shebang for your python standalone script          </li> </ul>"},{"location":"blog/tags/#tag:shortcuts","title":"shortcuts","text":"<ul> <li>            Paperless          </li> </ul>"},{"location":"blog/tags/#tag:synology","title":"synology","text":"<ul> <li>            Managing dockers using ansible on Synology NAS          </li> </ul>"},{"location":"blog/tags/#tag:taskfile","title":"taskfile","text":"<ul> <li>            Taskfile magic with docker[-]compose          </li> </ul>"},{"location":"blog/tags/#tag:un-static","title":"un-static","text":"<ul> <li>            Contact form made simple          </li> </ul>"},{"location":"blog/tags/#tag:uv","title":"uv","text":"<ul> <li>            Convert poetry to uv          </li> <li>            Improved shebang for your python standalone script          </li> </ul>"},{"location":"blog/tags/#tag:validation","title":"validation","text":"<ul> <li>            Pydantic series          </li> </ul>"},{"location":"blog/tags/#tag:website","title":"website","text":"<ul> <li>            Contact form made simple          </li> </ul>"},{"location":"blog/2025/05/27/convert-poetry-to-uv/","title":"Convert poetry to uv","text":"<p> When I encountered #uv I fell for it right away. I was already using ruff for a while and started looking into rye, which was the intermediate step before it became uv.</p> <p>Creating a uv repo is easy, just run <code>uv init $project</code> and you are off to the races. Using it for new projects is easy if you are already used to poetry. The commands are quite similar. Just add a few libraries with <code>uv add $library</code> or <code>uv add --dev $library</code> to make it part of the development group.</p> <p>Even loading libraries from your requirements file is easy, <code>uv add -r requirements.txt</code>.</p> <p>Now, you might want to migrate other repositories to have some consistency.  How to migrate your existing repo's to uv. Doing it by hand is just plain annoying and it will be easy to sneak in some errors.</p> <p>After I did that once, and imaging other would be in the same pickle. I thought it would be useful to have a migration script to convert the existing #poetry <code>pyproject.toml</code> to one that can be consumed by #uv.</p> <p>I looked for existing ones, but didn't find any that met my requirements or desires. So, nothing to do than creating my own. And so it was born. convert_poetry2uv</p> <p>to make it really easy to use, when you already have uv installed is just calling it directly:</p> <pre><code>uvx convert-poetry2uv -n pyproject.toml\n</code></pre> <p>The <code>-n</code> flag will do a dry run and creates a new file instead of moving the old one out of the way, allowing you to verify its contents. To do the actual migration, just:</p> <pre><code>uvx convert-poetry2uv pyproject.toml\n</code></pre> <p>(Your original file will be backed up)</p> <p>I hope this project will help you in your migrations and make them smooth and quick.</p> <p>In case you run into challenges, don't hesitate to reach out.</p>","tags":["python","poetry","uv"]},{"location":"blog/2025/06/13/taskfile-magic-with-docker-compose/","title":"Taskfile magic with docker[-]compose","text":"<p>I have been using Taskfile for a while now, and I really like it. It is a great alternative to Makefile, and it has some nice features that make it easier to use.</p> <p>In this post I wanted to share some \"magic\" to be able to select the right docker compose application to use. Either the older <code>docker-compose</code> or the newer <code>docker compose</code> command, using the compose plugin.</p> <p>I have a <code>Taskfile.yml</code> that looks like this that allows me to update running dockers, by finding them using <code>docker compose ls</code> and with those entries follow the path and pull the images and restart the containers.</p> <p>I want to be able to use the same tasks, even though I have to work with a system that still uses the older <code>docker-compose</code> command.</p>","tags":["taskfile","docker-compose","docker"]},{"location":"blog/2025/06/13/taskfile-magic-with-docker-compose/#how-to-deal-with-this","title":"How to deal with this?","text":"<p>In the <code>var</code> section of the <code>Taskfile.yml</code>, I define a variable <code>DOCKERCOMPOSE</code> that will be set to either <code>docker compose</code> or <code>docker-compose</code> depending on the availability of the command.</p> <p>The command is: <code>command -v docker-compose &amp;&gt; /dev/null &amp;&amp; echo \"docker-compose\" || echo \"docker compose\"</code></p> <p>The section would therefore result in:</p> <pre><code>vars:\n  DOCKERCOMPOSE:\n    sh: command -v docker-compose &amp;&gt; /dev/null &amp;&amp; echo \"docker-compose\" || echo \"docker compose\"\n</code></pre>","tags":["taskfile","docker-compose","docker"]},{"location":"blog/2025/06/13/taskfile-magic-with-docker-compose/#taskfile-usage","title":"Taskfile usage","text":"<p>This will allow you to use the <code>DOCKERCOMPOSE</code> variable in your tasks, like so:</p> <pre><code>tasks:\n  up:\n    dir: \"{{.USER_WORKING_DIR}}\"\n    preconditions:\n      - test -f docker-compose.yml\n    cmds:\n      - \"{{.SUDO}} {{.DOCKERCOMPOSE}} up -d --remove-orphans\"\n</code></pre> <p>I have the same kind of logic around <code>{{.SUDO}}</code>. If this environment variable is set, it will use <code>sudo</code> to run the command, otherwise it will just run the command without <code>sudo</code>. This therefore depends on the system I used it on.</p> <p>Essentially, this allows me to use the same <code>Taskfile.yml</code> on different systems, without having to change the commands or the tasks.</p>","tags":["taskfile","docker-compose","docker"]},{"location":"blog/2025/06/13/taskfile-magic-with-docker-compose/#more-tasks-around-docker-compose","title":"More tasks around docker compose","text":"<p>Below some more tasks that I use in my <code>Taskfile.yml</code>:</p> <pre><code>version: \"3\"\n\n# env SUDO=sudo may be set and will be used if so\n\nvars:\n  DOCKERCOMPOSE: \"docker compose\"\n\ntasks:\n  default:\n    cmd: task -l --sort none\n    silent: true\n\n  ### Per project tasks\n  up:\n    desc: \"Start the docker-compose environment\"\n    dir: \"{{.USER_WORKING_DIR}}\"\n    preconditions:\n      - test -f docker-compose.yml\n    cmds:\n      - \"{{.SUDO}} {{.DOCKERCOMPOSE}} up -d --remove-orphans\"\n\n  down:\n    desc: \"Stop the docker-compose environment\"\n    dir: \"{{.USER_WORKING_DIR}}\"\n    preconditions:\n      - test -f {{.USER_WORKING_DIR}}/docker-compose.yml\n    cmds:\n      - \"{{.SUDO}} {{.DOCKERCOMPOSE}} down\"\n\n  ps:\n    desc: \"List the containers in the docker-compose environment\"\n    dir: \"{{.USER_WORKING_DIR}}\"\n    preconditions:\n      - test -f {{.USER_WORKING_DIR}}/docker-compose.yml\n    cmds:\n      - \"{{.SUDO}} {{.DOCKERCOMPOSE}} ps\"\n\n  update:\n    desc: \"Update the docker-compose environment\"\n    dir: \"{{.USER_WORKING_DIR}}\"\n    preconditions:\n      - test -f {{.USER_WORKING_DIR}}/docker-compose.yml\n    cmds:\n      - \"{{.SUDO}} {{.DOCKERCOMPOSE}} pull\"\n\n  restart:\n    desc: \"Restart the docker-compose environment\"\n    dir: \"{{.USER_WORKING_DIR}}\"\n    preconditions:\n      - test -f {{.USER_WORKING_DIR}}/docker-compose.yml\n    cmds:\n      - \"{{.SUDO}} {{.DOCKERCOMPOSE}} restart\"\n\n  ### Global tasks\n  update-all:\n    desc: \"Update all docker-compose environments\"\n    aliases:\n      - update_all\n    preconditions:\n      - \"[ `uname -n` = nas ]\"\n    cmd: |\n      this_dir=$(pwd)\n      echo \"Get running {{.DOCKERCOMPOSE}} envs\"\n      running=$({{.SUDO}} {{.DOCKERCOMPOSE}} ls | grep running | awk '{print $1}')\n      for x in $running; do\n        cd $x\n        pwd\n        task update\n        task up\n        cd ${this_dir}\n        echo \"\"\n      done\n\n  restart-all:\n    desc: \"Restart all docker-compose environments\"\n    aliases:\n      - restart_all\n    preconditions:\n      - \"[ `uname -n` = nas ]\"\n    cmd: |\n      this_dir=$(pwd)\n      echo \"Get running {{.DOCKERCOMPOSE}} envs\"\n      running=$({{.SUDO}} {{.DOCKERCOMPOSE}} ls | grep running | awk '{print $1}')\n      for x in $running; do\n        cd $x\n        pwd\n        {{.SUDO}} {{.DOCKERCOMPOSE}} restart\n        cd ${this_dir}\n        echo \"\"\n      done\n\n  prune:images:\n    desc: \"Prune unused docker images\"\n    interactive: true\n    cmds:\n      - \"{{.SUDO}} docker image prune\"\n</code></pre>","tags":["taskfile","docker-compose","docker"]},{"location":"blog/2025/03/02/paperless/","title":"Paperless","text":"<p>Recently I was helping out some friends with paperless, how to structure their documents, how to implement a flow for their documents, etc. On occasion this triggered me to look at how I use paperless myself. I've been using it for a while now and I'm very happy with it. I've also added some automation to it, which I'll share in this post.</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#the-start","title":"The start","text":"<p>Getting started with paperless is quite easy. The documentation is good and will get you started quickly. I'm running it in a docker container, which is also described in the documentation. I'm running the <code>tika</code> variant with <code>postgres</code>. See the docker-compose.yml for the details.</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#beautiful-part","title":"Beautiful part","text":"<p>Let's skip right to the beautiful part. No matter how you start your setup with paperless, there is no wrong way. No matter how you start with tags, correspondents, document types or storage paths, you can always change them later. This is what I love about paperless. You can start with a simple setup and grow it as you go.</p> <p>Even the storage path structure, which is already very flexible in paperless, can be changed later. Any change to it, will be reflected on the filesystem without any worries on failing.</p> <p>Here are some examples of how I've structured my documents:</p> <p>Contracts: <pre><code>contracts/{{ created_year }}/{{ correspondent }}/{{ created_year }}{{ created_month }}{{ created_day }}_{{ title }}\n</code></pre></p> <p>Invoices private: <pre><code>private/invoices/{{ created_year }}/{{ correspondent }}/{{ created_year }}{{ created_month }}{{ created_day }}_{{ title }}\n</code></pre></p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#algorithms","title":"Algorithms","text":"<p>For all \"things\" in paperless, you can choose to have it done \"automagically\" or helping it out a little bit.</p> <p>As with the storage paths, the matching can be done using it algorithms, these will start working after learning about a few (if I recall correctly, ~ 20 documents).</p> <p>Otherwise, you can have it match on the presence of certain words, either exact or via regex, and some more options. I'd say, there is no best way, just your way of choosing how to use it.</p> <p>The same logic applies to <code>tags</code>, <code>document types</code> and <code>correspondents</code>.</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#automation","title":"Automation","text":"<p>I'll go into some of the internal automation as well as some external ones.</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#custom-fields-and-workflows","title":"Custom fields and workflows","text":"<p>For the invoices I've created a custom field <code>Total</code>. This is a <code>Monetary</code> field. I want this field to be present on any invoice.</p> <p>This can be done manually, but where is the fun in that?</p> <p>I've created a workflow that add the <code>total</code> field to the respective document type automatically. This is done by a trigger when a documented is added to the <code>invoices</code> category or when it is updated to it.</p> <p></p> <p>See an example below, note that <code>Factuur</code> is Dutch for <code>Invoice</code>.</p> <p></p> <p>In the \"Actions\" section you can assign the custom field <code>Total</code> to the document.</p> <p></p> <p>Note: What this doesn't do: When it was incorrectly assigned to invoices, it will not remove the custom field. Though this is still easily done when processing your inbox.</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#email","title":"Email","text":"<p>I have set up a dedicated mailbox for paperless to monitor. Every hour paperless scan the mailbox for new mails. It will process the documents and add them to the database.</p> <p>With this in place, I can just send/forward any mail. Quite useful! I didn't want to give it access to my private or work mailboxes, that is why I've setup up a dedicated mailbox.</p> <p>Having the dedicated mailbox I can have some additional automation for it. When invoices are received to a specific alias, they will also be forwarded to my bookkeeping program, allowing me to process that email in both systems with a single action.</p> <p>Therefore, depending on the alias, the document will be processed differently.</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#paperless-app-the-phone","title":"Paperless app the phone","text":"<p>I've also set up the paperless app on my phone. This allows me to quickly scan documents and have them processed by paperless. I can also add tags, correspondents and categories to the document. This is quite useful when I'm on the go and want to process a document quickly.</p> <p>Sharing documents from the browser is not really a quick option, you would need to download the file and take an action on it.</p> <p>With the application you can look at a document and quickly take an action on it. Including having your own shortcuts..</p> <p>(I'm sure there is an app on Android as well)</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#shortcuts","title":"Shortcuts","text":"<p>I've created some shortcuts on my phone to quickly add a document to paperless or to email them to the different mail addresses for the various actions.</p> <p>With these in place I'm able to quickly process documents, from the phone, tablet or laptop.</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#add-document-to-paperless-with-the-api","title":"Add document to paperless with the API","text":"<p>Here is a public shortcut to add a file/pdf/image to paperless using the api. On the first run you will need to set the API key and the URL to your paperless instance.</p> <p>This might be less useful on the phone, where you have the app, but it might be useful on the tablet or laptop.</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#email-documents-with-quick-actions","title":"Email documents with quick actions","text":"<p>Here is my shortcut for sending a file to an email list, where you can select which predefined email address it goes to.</p> <p>This allows me to use the quickactions on the phone, tablet or laptop to send an email address of choice.</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#missing-features","title":"Missing features?","text":"<p>With all this greatness in place, what could possible be missing?</p> <p>Having a forward/send button or something like a quick action available within paperless would be awesome to have. This would allow me to quickly forward a document to my bookkeeping program as an example.</p> <p>Though, even without this feature, I'm very happy with paperless and the automation I've added to it.</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/08/29/pydantic-series/","title":"Pydantic series","text":"<p>The focus in this blog is on validation, as well as being able to return the same output as we got the input. Which is a json format. As an example, when parsing an IP address as a string, the output value should be a string again and not an ipaddress object. Or the MAC address which needs to retain a certain format.</p> <p>The \"bonus\" of also dumping it back into json, is that a diff can be done between the original and the output to see if anything was changed or missed.</p> <p>This is based on a series that I did on LinkedIn.</p>","tags":["pydantic","data validation","validation"]},{"location":"blog/2025/08/29/pydantic-series/#basics-first","title":"Basics first","text":"<p>Let's start with some basics.</p> <p></p> <p>A print on 'user' and 'user.name' shows:</p> <pre><code>\"id=1 name='John Doe' email='some@email.com'\"\nand\n\"John Doe\"\n</code></pre> <p>The same can be done with a user list and therefore using nested classes. See the 2nd screenshot</p> <p>The output of \"user\" becomes:</p> <pre><code>\"users=[User(id=1, name='John Doe', email='some@email.com'), User(id=2, name='Jane Foo', email='jane@foo.com')]\"\n</code></pre> <p></p> <p>Basics code on Github</p>","tags":["pydantic","data validation","validation"]},{"location":"blog/2025/08/29/pydantic-series/#stricter-validation","title":"Stricter validation","text":"<p>Here I'm coming from the angle that strict validation is better. It should fail sooner than later on possible wrong values.</p> <p>Validation is done on a positive integer for the ID, the name string should not be empty and the email, should be.... a valid email of course. We will use the pydantic library to achieve this.</p> <p>If you already had pydantic installed, it might still complain about the email portion. Add it using uv <code>uv add pydantic[email]</code> or by <code>pip install pydantic[email]</code>.</p> <p>Looking at the first code the result is also shown. The happy flow is the easiest. But what if we \"mess up\"...</p> <p></p> <p>It will show a traceback on the wrong data, in this case the email address.</p> <p></p> <p>A wrong ID (negative) is also shown.</p> <p></p> <p>But... What if the ID is not a integer, will it fail? It actually doesn't, pydantic will tolerate this and will try to parse this string as an int, because that is what we told it to do. This is great, because that is what you wanted, right?</p> <p></p> <p>Though, it is not that great when you actually need to convert it back to its original state, meaning you want the validation but not actually changing the data because this model should work both ways.</p> <p>This is what will be covered in the next chapter.</p> <p>Stricter code on Github</p>","tags":["pydantic","data validation","validation"]},{"location":"blog/2025/08/29/pydantic-series/#additional-validation","title":"Additional validation","text":"<p>This chapter will handle some custom fields, being specific using \"Literal\", but also dealing with \"Optional\" fields.</p> <p>All of this still needs to be able to convert back in the same way it was written. Because of this I'll also introduce the \"deepdiff\" library. Allowing me to compare the in and output, to validate them. This also allows me to capture any field that I could've missed, which would not show up in the output.</p> <p>Let's start of with some pieces. Instead of using a PositiveInt as we did last time. We use a Annotated object, it will take an int and we still check for its value to be &gt; 1. It gives me the option to add a Serializer allowing me to take actions when dumping it back into a json.</p> <p></p> <p>Annotated is from typing, PlainSerializer is from the #pydantic library.</p> <pre><code>IntStr = Annotated[int, PlainSerializer()]\n</code></pre> <p>In the example, we wish to parse a mac address. it is given in the format: \"00:1A:2B:3C:4D:5E\". We can use the <code>netaddr</code> library and use the EUI class, though this will convert it to \"00-1A-2B-3C-4D-5E\". This would be a problem when converting it back to a json.</p> <p></p> <p>Let's create another Annotated object, which will do the validation, but not change the format on how it is stored.</p> <pre><code>MacAddress = Annotated[\n    str,\n    AfterValidator(mac_address_validator),\n]\n</code></pre> <p>The type of the value is consider a string and we use the AfterValidator to ensure the format is as expected, but not storing its original value.</p> <p>Have a look at the full code and its output.</p> <p></p> <p>Next to the two snippets shown above, we also introduced Literal, which takes exact values which should be match. If it ain't defined, it ain't accepted. The Optional is seen on the rack. Where the key may be omitted, though if provided it should be a string. If absent, the default value is None.</p> <p>Comparing the 2 objects by hand doesn't sound like fun. Introducing deepdiff to compare 2 objects, as well as rich.print to improve readability . (2nd screenshot)</p> <p></p> <p>Given the code from the first screenshot, we still have issues. The most tricky one to see is the \"instance-id\" key, which got changed to \"instance_id\". See:</p> <pre><code>instance_id: IntStr = Field(alias=\"instance-id\")\n</code></pre> <p>the reading parsing went fine, dumping the model not so much. Easy fix:</p> <p><code>devices.model_dump(by_alias=True)</code></p> <p>same would apply for devices.model_dump_json(by_alias=True)</p> <p>The other entry in the list has the \"rack\" key defined, because we gave it a default value \"None\". This is not what I want, so also adding <code>exclude_none=True</code> to the dump.</p> <p>We now have a repeatable test , which can also validate if fields would've been added to the json, but not in the model.</p> <p></p> <p>A perfect validation in your pipeline...</p> <p>Annotations code on Github</p>","tags":["pydantic","data validation","validation"]},{"location":"blog/2025/08/29/pydantic-series/#a-bit-of-regex","title":"A bit of regex","text":"<p>The last time I showed a custom AfterValidator for validating a MacAddress. Let's have a look at a port validator as well. This specific solution is meant for a <code>Juniper</code> device.</p> <p>It is expected a string and it shouldn't be empty, furthermore it should comply with the provided regex. This validator doesn't affect how the value is stored, its sole purpose is to validate the input matching a port.</p> <p>The following values are matching entries:</p> <pre><code>et-0/0/0\nge-0/0/1:1\nge-1/0/1:15\nxe-2/2/2\n</code></pre> <p></p> <p>Regex code on Github</p>","tags":["pydantic","data validation","validation"]},{"location":"blog/2025/08/29/pydantic-series/#union","title":"Union","text":"<p>This chapter deals with values that could be different types. On its own it is not that hard, especially using a newer #python version.</p> <p>The code and output show the use of multiple value types used. \"role\" could be a string or a list of strings.</p> <p></p> <p>While \"addr\" is an optional field, which can be a IPv4Interface or a list of them, or a literal empty string. It is not ideal, but being able to deal with this type of things is a real world example.</p> <p>This is using the \"smart mode\" of Unions.</p> <p>NOTE:</p> <p>Here is a heads up in case you were trying to do the same trick, but then using \"ip_interface\", which could handle IPv4Interface and IPv6Interface and return the correct type.... It doesn't work.</p> <p>Changing the addr line: <code>addr: Optional[ip_interface | list[IPv4Interface] | Literal[\"\"]] = None</code></p> <p>And running it gives:</p> <pre><code>Traceback (most recent call last):\n  File \"/Users/bart/git/pydantic_series/05_double_typed_data/01_double_typed.py\", line 9, in &lt;module&gt;\n    class NetworkDevice(BaseModel):\n    ...&lt;3 lines&gt;...\n        addr: Optional[ip_interface | list[IPv4Interface] | Literal[\"\"]] = None\n  File \"/Users/bart/git/pydantic_series/05_double_typed_data/01_double_typed.py\", line 13, in NetworkDevice\n    addr: Optional[ip_interface | list[IPv4Interface] | Literal[\"\"]] = None\n                   ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~\nTypeError: unsupported operand type(s) for |: 'function' and 'types.GenericAlias'\n</code></pre> <p>This is due to it being a <code>function</code>, not a <code>Type</code>.</p>","tags":["pydantic","data validation","validation"]},{"location":"blog/2025/08/29/pydantic-series/#option-2","title":"Option 2","text":"<p>An alternative is using the \"IPvAnyInterface\" from pydantic instead.</p> <pre><code>addr: Optional[IPvAnyInterface | list[IPvAnyInterface] | Literal[\"\"]] = None\n</code></pre> <p>This works great for validation, though when writing back a json, the type is still present:</p> <pre><code>\"addr\": \"[IPv4Interface('192.168.1.1/24')]\"\n</code></pre> <p>We could solve all of this with the <code>serialiser</code>, or we stick to the specific types. Choices, choices...</p> <p>For the deepdiff check, even with the initial method, we will change it to using a dump to json and then reading the json back into a python object. This will eliminate the types that would otherwise conflict. As an example:</p> <pre><code>    'values_changed': {\"root['devices'][0]['addr'][0]\": {'new_value': IPv4Interface('192.168.1.1/24'), 'old_value': '192.168.1.1/24'}}\n</code></pre> <p>See the second screenshot with the end result being the same as the input. Mission accomplished.</p> <p></p> <p>I do realize that my additional request to be able to write the json the same as it was can be a challenging requirement and most that read this, might have a simpler use case, which just uses validation. If so, make your life easier by simplifying it all.</p> <p>Though for those that are in some sort of migration that needs to go both ways, this might be the trick you were looking for. This also allows for validation if all the data is actually modeled. If any field is missing in pydantic, it will present a diff in the result.</p> <p>Double typed data code on Github</p>","tags":["pydantic","data validation","validation"]},{"location":"blog/2025/08/29/pydantic-series/#dynamic-dictionary","title":"Dynamic dictionary","text":"<p>In the previous examples the structure was easily matched on the model and something can be said about that. If you can influence that, make it so! Your future self will thank you.</p> <p>In this example, the keys are \"dynamic\" in the way, we can't predict them. Instead of using the pydantic <code>BaseModel</code>, we'll use the <code>RootModel</code> to match this.</p> <p></p> <p>The trick is in the <code>DynamicDict</code> class, with the <code>RootModel</code> defined.</p> <pre><code>class DynamicDict(RootModel[dict[str, NetworkDevice]]):\n    pass\n</code></pre> <p>The <code>NetworkDevice</code> is the same as in previous example.</p> <p>But.... Could it deal with this kind of dynamic nature, when the inner dictionary is also dynamic (read: different) per key.</p> <p>This is the part where I saw some real power of pydantic. I expected this would've led to some complicated code. Turns out, it is just one (smart) Union away. See screenshot 2.</p> <p></p> <p>We \"just\" created another model (<code>OtherDevice</code>) and added it to the <code>DynamicDict</code> class.</p> <pre><code>class DynamicDict(RootModel[dict[str, NetworkDevice | OtherDevice]]):\n</code></pre> <p>Man, I love this smart Union mode.</p> <p>Dict root code on Github</p>","tags":["pydantic","data validation","validation"]},{"location":"blog/2025/08/29/pydantic-series/#own-validator","title":"Own validator","text":"<p>There are limits...</p> <p>In the previous post, I showed the power of smart Union. But if your data is complex enough, the Union might get confused. The following example will show what I'm hinting at, though this example will not crash on making it a union.</p> <p>It is still meant as an alternative for those \"rare\" (daily average production) occasions.</p> <p>This example (man I do hope you have a tall monitor) has some inheritance to simplify some models and the main part which is:</p> <pre><code>NETWORK_DEVICE_REGISTRY = {\n    \"rtr\": NetworkDeviceRtr,\n    \"switch\": NetworkDeviceSwitch,\n    \"console\": NetworkDeviceConsole,\n}\n\n\nclass NetworkDeviceDict(dict[str, BaseModel]):\n    @classmethod\n    def __get_validators__(cls) -&gt; Iterator:\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, value: dict[str, dict], info=None) -&gt; \"NetworkDeviceDict\":\n        if not isinstance(value, dict):\n            raise TypeError(\"devices must be a dict\")\n        result = {}\n        for key, val in value.items():\n            if key.startswith(\"rtr\"):\n                key_name = \"rtr\"\n            elif key.startswith(\"switch\"):\n                key_name = \"switch\"\n            elif key.startswith(\"console\"):\n                key_name = \"console\"\n            else:\n                raise ValueError(f\"key '{key}' not lookup logic for devices\")\n            model_class = NETWORK_DEVICE_REGISTRY.get(key_name)\n            if not model_class:\n                raise ValueError(f\"key '{key}' not in NETWORK_DEVICE_REGISTRY\")\n            try:\n                result[key] = model_class(**val)\n            except ValidationError as e:\n                pprint(f\"Validation error for {key}: {e}\")\n        return cls(result)\n</code></pre> <p>With its own <code>validator</code> it allows you to specifically select which model to assign to which key. This would be particularly useful for complex data structures where you might want to select on something deeper down the tree. Or just providing the different type of models based on the key, as shown in this example.</p> <p></p> <p>Own validator code on Github</p>","tags":["pydantic","data validation","validation"]},{"location":"blog/2025/08/29/pydantic-series/#before-validator","title":"Before validator","text":"<p>In a chat, Urs Baumann pointed out to be annoyed by inputs that could have 2 values, like strings and list of strings. To store those consistently as list[str] inside your pydantic model, the BeforeValidator() can be used. This will manipulate your data into its consistent desired type.</p> <p>I hadn't written about it before, because my specific use case required data to be restored as it was, therefore I was using a Union of \"str | list[str]\". The output below shows that the BeforeValidator makes things consistent, therefore it can't be restored to its original type (str).</p> <p>Treat this as a different use case, ensuring consistency.</p> <p></p> <p>The diff for the return path is beautifully shown by the deepdiff output.</p> <pre><code>{\n    'type_changes': {\n        \"root['random2']['role']\": {\n            'old_type': &lt;class 'str'&gt;,\n            'new_type': &lt;class 'list'&gt;,\n            'old_value': 'access',\n            'new_value': ['access']\n        },\n        \"root['random3']['role']\": {\n            'old_type': &lt;class 'str'&gt;,\n            'new_type': &lt;class 'list'&gt;,\n            'old_value': 'core',\n            'new_value': ['core']\n        }\n    }\n}\n</code></pre> <p>BeforeValidator code on Github</p>","tags":["pydantic","data validation","validation"]},{"location":"blog/2025/08/29/pydantic-series/#conclusion","title":"Conclusion","text":"<p>In this long post, we explored the use of Pydantic to validate complex data structures, including the use of custom validators and the BeforeValidator to ensure consistent data types. By leveraging these features, we can create more robust and maintainable data models.</p> <p>By having this data verified upfront, we can catch potential issues early in the process, leading to a simpler and more efficient code focussing on the business logic.</p> <p>Do note that this specific use case of being able to dump the data in exactly the same format as it was received is usually not required and simplifies the overall data handling process.</p>","tags":["pydantic","data validation","validation"]},{"location":"blog/2025/01/07/pypi-trusted-publisher-management-and-pitfalls/","title":"Pypi Trusted Publisher Management and pitfalls","text":"<p> Be the cool kid on pypi, I thought, use the Trusted Publisher Management and OpenID Connect (OIDC)... I thought...</p> <p>While working on my latest repo convert_poetry2uv, I wanted to automatically push the builds to pypi. Traditionally a username/password combination was used, which was later replaced by an API token. These days OIDC can be used, which I tried. I'm here to share some pitfalls, so hopefully you don't fall in them.</p>","tags":["pypi","python","oidc","github-actions"]},{"location":"blog/2025/01/07/pypi-trusted-publisher-management-and-pitfalls/#the-beginning","title":"The beginning","text":"<p>Let's start with the guides I followed:</p> <p>https://packaging.python.org/en/latest/guides/publishing-package-distribution-releases-using-github-actions-ci-cd-workflows/</p> <p>This is actually quite clear and will get you started quickly, with also good examples. I only changed the actions yaml a bit, to work with uv.</p> <p>Another doc is by pypi, https://docs.pypi.org/trusted-publishers/creating-a-project-through-oidc/</p> <p>So, let's start with which field needs to be what.</p> <p>This is what is currently being used for my test flow, to push to test.pypi.org.</p> <p></p> <p>Create a new publisher</p>","tags":["pypi","python","oidc","github-actions"]},{"location":"blog/2025/01/07/pypi-trusted-publisher-management-and-pitfalls/#any-caveats","title":"Any caveats?","text":"<p>So what are the caveats, this looks simple enough, right?</p>","tags":["pypi","python","oidc","github-actions"]},{"location":"blog/2025/01/07/pypi-trusted-publisher-management-and-pitfalls/#typos","title":"Typos","text":"<p>obviously!</p>","tags":["pypi","python","oidc","github-actions"]},{"location":"blog/2025/01/07/pypi-trusted-publisher-management-and-pitfalls/#project-name","title":"Project name","text":"<p>The next thing is the PyPi Project Name. This needs to match the name defined in your <code>pyproject.toml</code> (no underscore, but hyphens!). If it doesn't match you'll get 403 errors when trying to upload.</p> 403 error<pre><code>  &lt;title&gt;403 Invalid API Token: OIDC scoped token is not valid for\n    project 'poetry-to-uv', project-scoped token is not valid for project:\n    'poetry-to-uv'&lt;/title&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n      &lt;h1&gt;403 Invalid API Token: OIDC scoped token is not valid for project\n    'poetry-to-uv', project-scoped token is not valid for project:\n    'poetry-to-uv'&lt;/h1&gt;\n      Access was denied to this resource.&lt;br/&gt;&lt;br/&gt;\n    Invalid API Token: OIDC scoped token is not valid for project\n    &amp;#x27;poetry-to-uv&amp;#x27;, project-scoped token is not valid for\n    project: &amp;#x27;poetry-to-uv&amp;#x27;\n</code></pre> <p>NOTE: this error still has the old project name, which was part of my original challenge</p>","tags":["pypi","python","oidc","github-actions"]},{"location":"blog/2025/01/07/pypi-trusted-publisher-management-and-pitfalls/#-vs-_","title":"\"-\" vs \"_\"","text":"<p>During my initial steps I also ran into a invalid-publisher. This is described on the troubleshooting page.</p> <p>In my case, this was a mismatch between \"_\" and \"-\".</p>","tags":["pypi","python","oidc","github-actions"]},{"location":"blog/2025/01/07/pypi-trusted-publisher-management-and-pitfalls/#not-a-problem","title":"Not a problem","text":"<p>Having a repository name different than the project name is not a problem, as long as they are specified correctly on the pypi.org publishing page.</p>","tags":["pypi","python","oidc","github-actions"]},{"location":"blog/2025/01/07/pypi-trusted-publisher-management-and-pitfalls/#example-highlights","title":"Example highlights","text":"<p>At the bottom of this article is my complete <code>publish-to-pypi.yml</code> workflow.</p> <p>Here are some highlight on the snippets.</p> environment<pre><code> publish-to-testpypi:\n    name: Upload release to TestPyPI\n    needs:\n      - build\n    runs-on: ubuntu-latest\n\n    environment:\n      name: testpypi\n      url: https://test.pypi.org/p/convert-poetry2uv\n</code></pre> <p>Note the name of the environment, it matches the environment that is configured in the publisher on pypi.org.</p> repository-url<pre><code> - name: Publish distribution \ud83d\udce6 to TestPyPI\n        uses: pypa/gh-action-pypi-publish@release/v1\n        with:\n          repository-url: https://test.pypi.org/legacy/\n          verbose: true\n          skip-existing: true\n</code></pre> <p>For TestPyPi the repository-url needs to be set. I've set the verbose to true, so I was able to inspect my issues. The skip-existing flag is useful for testing on TestPyPi. It will not overwrite the existing file with the same version number, but it will not fail the pipeline either.</p>","tags":["pypi","python","oidc","github-actions"]},{"location":"blog/2025/01/07/pypi-trusted-publisher-management-and-pitfalls/#full-example","title":"Full example","text":"<p>For the full section below.</p> <ul> <li>Only push to pypi when a new tag is set</li> </ul> publish-to-pypi<pre><code>  publish-to-pypi:\n    name: Publish Python \ud83d\udc0d distribution \ud83d\udce6 to PyPI\n    if: startsWith(github.ref, 'refs/tags/') # only publish to PyPI on tag pushes\n</code></pre> <ul> <li>${{ github.token }}, is automatically set. No need to worry about it.</li> </ul> <pre><code> - name: Create GitHub Release\n    env:\n      GITHUB_TOKEN: ${{ github.token }}\n</code></pre> <ul> <li>With the upload to pypi successful, a release is created also on github.</li> </ul> publish-to-pypi.yml<pre><code>name: Publish to TestPyPI\n\non: push\n\njobs:\n  build:\n    name: Build distribution \ud83d\udce6\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v5\n        with:\n          # Install a specific version of uv.\n          version: \"0.5.13\"\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.12\"\n\n      - name: build distribution\n        run: uv build\n\n      - name: Store the distribution packages\n        uses: actions/upload-artifact@v4\n        with:\n          name: python-package-distributions\n          path: dist/\n\n  publish-to-testpypi:\n    name: Upload release to TestPyPI\n    needs:\n      - build\n    runs-on: ubuntu-latest\n\n    environment:\n      name: testpypi # (1)!\n      url: https://test.pypi.org/p/convert-poetry2uv\n\n    permissions:\n      id-token: write # IMPORTANT: mandatory for trusted publishing\n\n    steps:\n      - name: Download all the dists\n        uses: actions/download-artifact@v4\n        with:\n          name: python-package-distributions\n          path: dist/\n      - name: Publish distribution \ud83d\udce6 to TestPyPI\n        uses: pypa/gh-action-pypi-publish@release/v1\n        with:\n          repository-url: https://test.pypi.org/legacy/\n          verbose: true\n          skip-existing: true\n\n  publish-to-pypi:\n    name: Publish Python \ud83d\udc0d distribution \ud83d\udce6 to PyPI\n    if: startsWith(github.ref, 'refs/tags/') # (3)! only publish to PyPI on tag pushes\n    needs:\n      - build\n    runs-on: ubuntu-latest\n\n    environment:\n      name: pypi # (2)!\n      url: https://pypi.org/p/convert-poetry2uv\n\n    permissions:\n      id-token: write # IMPORTANT: mandatory for trusted publishing\n\n    steps:\n      - name: Download all the dists\n        uses: actions/download-artifact@v4\n        with:\n          name: python-package-distributions\n          path: dist/\n      - name: Publish distribution \ud83d\udce6 to PyPI\n        uses: pypa/gh-action-pypi-publish@release/v1\n\n  github-release:\n    name: &gt;-\n      Sign the Python \ud83d\udc0d distribution \ud83d\udce6 with Sigstore\n      and upload them to GitHub Release\n    needs:\n      - publish-to-pypi\n    runs-on: ubuntu-latest\n\n    permissions:\n      contents: write # IMPORTANT: mandatory for making GitHub Releases\n      id-token: write # IMPORTANT: mandatory for sigstore\n\n    steps:\n      - name: Download all the dists\n        uses: actions/download-artifact@v4\n        with:\n          name: python-package-distributions\n          path: dist/\n      - name: Sign the dists with Sigstore\n        uses: sigstore/gh-action-sigstore-python@v3.0.0\n        with:\n          inputs: &gt;-\n            ./dist/*.tar.gz\n            ./dist/*.whl\n      - name: Create GitHub Release\n        env:\n          GITHUB_TOKEN: ${{ github.token }} # (4)!\n        run: &gt;-\n          gh release create\n          \"$GITHUB_REF_NAME\"\n          --repo \"$GITHUB_REPOSITORY\"\n          --notes \"\"\n      - name: Upload artifact signatures to GitHub Release # (5)!\n        env:\n          GITHUB_TOKEN: ${{ github.token }}\n        # Upload to GitHub Release using the `gh` CLI.\n        # `dist/` contains the built packages, and the\n        # sigstore-produced signatures and certificates.\n        run: &gt;-\n          gh release upload\n          \"$GITHUB_REF_NAME\" dist/**\n          --repo \"$GITHUB_REPOSITORY\"\n</code></pre> <ol> <li>Note the name of the environment, it matches the environment that is configured in the publisher on test.pypi.org.</li> <li>Note the name of the environment, it matches the environment that is configured in the publisher on pypi.org.</li> <li>Only push to pypi when a new tag is set</li> <li>${{ github.token }}, is automatically set. No need to worry about it.</li> <li>With the upload to pypi successful, a release is created also on github</li> </ol> <p>Have fun.</p>","tags":["pypi","python","oidc","github-actions"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/","title":"Managing dockers using ansible on Synology NAS","text":"<p>Synology NAS devices are great and easy to use devices. The model I use is plenty strong to running a few containers as well. However, the Synology Docker application is not the best when it comes to managing multiple containers. It is a bit clunky and does not provide the flexibility I want.</p> <p>Managing containers is very well possible using Ansible. Especially when you already use it for different systems and different tasks. Adding a playbook to allow you to update or manage your containers is a no-brainer.</p> <p>Let's start with the setup, and leave synology aside for a moment.</p> <p>I wanted to extend my playbook for updating my servers, including the containers running on them.</p>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#the-playbook","title":"The playbook","text":"<p>I have added the following part to my playbook:</p> <pre><code>- name: Update docker compose\n  hosts: dockerCompose\n  become: true\n  roles:\n    - { role: docker_compose, tags: docker_compose }\n</code></pre> <p>This will run the <code>docker_compose</code> role on the <code>dockerCompose</code> hosts. The <code>become: true</code> allows the playbook to run with elevated privileges, which is required to manage docker containers.</p>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#the-hosts","title":"The hosts","text":"<p>The hosts are defined in my <code>hosts.yaml</code> file:</p> <pre><code>dockerCompose:\n  hosts:\n    hostname1:\n    hostname2:\n</code></pre>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#the-role-docker_compose","title":"The role <code>docker_compose</code>","text":"<p>The role <code>docker_compose</code> is defined in my Ansible roles directory. The role contains the tasks to manage the docker containers, using the docker_compose_v2 module. The <code>roles/docker_compose/tasks/main.yml</code> looks like this:</p> <pre><code>---\n- name: Update dockers\n  become: true\n  community.docker.docker_compose_v2:\n    docker_cli: \"{{ docker_cli | default(omit) }}\"\n    project_src: \"{{ item.path }}\"\n    remove_orphans: true\n    state: restarted\n  loop: \"{{ dockers }}\"\n</code></pre> <p>This tasks will loop over the <code>dockers</code> variable, which is defined in the <code>hostvars</code>. This will have the paths to the docker compose files that need to be managed.</p> <pre><code>dockers:\n  - path: /path/to/project/holding/docker-compose_file\n  - path: /path/to/project1/holding/docker-compose_file\n  - path: /path/to/project2/holding/docker-compose_file\n</code></pre> <p>The <code>state: restarted</code> will ensure that the containers are restarted after pulling the latest images. The <code>remove_orphans: true</code> will remove any containers that are no longer defined in the docker compose file.</p> <p>The <code>docker_cli</code> variable is optional and can be used to specify the docker CLI command to use. If not set, it will use the default <code>docker compose</code> command. For my synology I had to specify it.</p> <p>Specify the <code>docker_cli</code> variable in the <code>host_vars</code> file for the Synology NAS:</p> <pre><code>docker_cli: /usr/local/bin/docker\n</code></pre>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#docker-compose-plugin","title":"Docker Compose Plugin","text":"<p>In the past, I had to do certain modifications to the synology python environment to be able to achieve this. Now I have modified the setup to use the compose plugin. Especially, that since 2022 the <code>docker-compose</code> command is deprecated and replaced by the <code>docker compose</code> command. The <code>community.docker.docker_compose_v2</code> module will use the newer <code>docker compose</code> command, based on the plugin.</p> <p>This plugin needs to be installed on the Synology NAS. I have created the following task to install the plugin.</p> <p>In the same role, <code>roles/docker_compose/tasks/synology.yml</code>:</p> <pre><code>---\n- name: Create a docker plugins directory\n  ansible.builtin.file:\n    path: \"{{ docker_plugins_path }}\"\n    state: directory\n    mode: \"0755\"\n\n- name: Download docker compose plugin\n  ansible.builtin.get_url:\n    url: https://github.com/docker/compose/releases/download/{{ docker_compose_version }}/docker-compose-linux-x86_64\n    dest: \"{{ docker_plugins_path }}/docker-compose\"\n    mode: \"0755\"\n</code></pre> <p>The following are added to the <code>hostvars</code>:</p> <pre><code>docker_compose_version: v2.37.1\ndocker_plugins_path: /usr/local/lib/docker/cli-plugins\n</code></pre>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#playbook-addition","title":"Playbook addition","text":"<p>To call this task, we add the following to the playbook, before calling the <code>docker_compose</code> role:</p> <pre><code>- name: Docker compose for Synology hosts\n  hosts: synology\n  become: true\n  tasks:\n    - name: Include Synology tasks\n      ansible.builtin.include_tasks: roles/docker_compose/tasks/synology.yml\n</code></pre>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#conclusion","title":"Conclusion","text":"<p>Using Ansible to manage docker containers on a Synology NAS is a great way to automate the management of your containers. It allows you to easily update and manage your containers, without having to use the Synology Docker application. The <code>community.docker.docker_compose_v2</code> module provides a simple way to manage your containers using the newer <code>docker compose</code> command, which is the recommended way to manage docker containers.</p>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#complete-files","title":"Complete files","text":"","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#update_serversyaml-playbook","title":"update_servers.yaml playbook","text":"<pre><code>---\n- name: Docker compose for Synology hosts\n  hosts: synology\n  become: true\n  tasks:\n    - name: Include Synology tasks\n      ansible.builtin.include_tasks: roles/docker_compose/tasks/synology.yml\n\n- name: Update docker compose\n  hosts: dockerCompose\n  become: true\n  roles:\n    - { role: docker_compose, tags: docker_compose }\n</code></pre>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#rolesdocker_composetasksmainyml","title":"roles/docker_compose/tasks/main.yml","text":"<pre><code>---\n- name: Install dependencies\n  ansible.builtin.command:\n    cmd: \"{{ item.install_command }}\"\n    chdir: \"{{ item.path }}\"\n  loop: \"{{ dockers }}\"\n  when: item.install_command is defined\n  register: my_output\n  changed_when: my_output.rc != 0 # &lt;- Uses the return code to define when the task has changed.\n  become_user: \"{{ user.name }}\"\n  become: true\n\n- name: Update dockers\n  become: true\n  community.docker.docker_compose_v2:\n    docker_cli: \"{{ docker_cli | default(omit) }}\"\n    project_src: \"{{ item.path }}\"\n    remove_orphans: true\n    state: present\n    ignore_build_events: false\n  loop: \"{{ dockers }}\"\n</code></pre>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#rolesdocker_composetaskssynologyyml","title":"roles/docker_compose/tasks/synology.yml","text":"<pre><code>---\n- name: Create a docker plugins directory\n  ansible.builtin.file:\n    path: \"{{ docker_plugins_path }}\"\n    state: directory\n    mode: \"0755\"\n\n- name: Download docker compose plugin\n  ansible.builtin.get_url:\n    url: https://github.com/docker/compose/releases/download/{{ docker_compose_version }}/docker-compose-linux-x86_64\n    dest: \"{{ docker_plugins_path }}/docker-compose\"\n    mode: \"0755\"\n</code></pre>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#host_varshostname1mainyaml","title":"host_vars/hostname1/main.yaml","text":"<pre><code>dockers:\n  - path: /path/to/project/holding/docker-compose_file\n  - path: /path/to/project1/holding/docker-compose_file\n  - path: /path/to/project2/holding/docker-compose_file\n\ndocker_cli: /usr/local/bin/docker\ndocker_compose_version: v2.37.1\ndocker_plugins_path: /usr/local/lib/docker/cli-plugins\n</code></pre>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#hostsyaml","title":"hosts.yaml","text":"<pre><code>dockerCompose:\n  hosts:\n    hostname1:\n    # hostname2:\n\nsynology:\n  hosts:\n    hostname1:\n      ansible_host: 192.168.3.20\n</code></pre>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/01/06/contact-form-made-simple/","title":"Contact form made simple","text":"<p> Today I ran into a service that allows you to create a contact form without having to write a bunch of code. Making it really easy to incorporate it in your mkdocs website.</p>","tags":["website","un-static"]},{"location":"blog/2025/01/06/contact-form-made-simple/#un-static-forms","title":"Un-static forms","text":"<p>The service is called Un-static and it allows you to create a form by registering a form on their website. Essentially, you don't even need to create an account unless you need some additional features.</p> <p>Once you create a form you can use the form's URL to submit the form. The form is submitted via a POST request and the form data is sent to the email address you specified when  the form was created.</p> <p>The how-to page on their website explains how to create a static form and how to use it.</p>","tags":["website","un-static"]},{"location":"blog/2025/01/06/contact-form-made-simple/#example","title":"Example","text":"<p>Below is an example of a form that I created using Un-static in combination with Mkdocs.</p> docs/contact/index.md<pre><code>---\nhide:\n    - navigation\n    - toc\n---\n&lt;section&gt;\n  &lt;div class=\"md-grid\"\n  style=\"max-width: 840px; margin-left: 0; display: flex; justify-content: left;\"&gt;\n    &lt;form method=\"post\"\n    action=\"https://forms.un-static.com/forms/&lt;form-id&gt;\"\n    class=\"md-grid md-form\"\n    style=\"display: grid; grid-template-columns: 150px 2fr; gap: 10px; width: 100%;\"&gt;\n        &lt;label for=\"name\" class=\"md-text\" style=\"text-align: left; padding-right: 10px;\"&gt;Name&lt;/label&gt;\n        &lt;input type=\"text\" id=\"name\" name=\"name\" required class=\"md-input\"&gt;\n\n        &lt;label for=\"email\" class=\"md-text\" style=\"text-align: left; padding-right: 10px;\"&gt;Email&lt;/label&gt;\n        &lt;input type=\"email\" id=\"email\" name=\"email\" required class=\"md-input\"&gt;\n\n        &lt;label for=\"subject\" class=\"md-text\" style=\"text-align: left; padding-right: 10px;\"&gt;Subject&lt;/label&gt;\n        &lt;input type=\"text\" id=\"subject\" name=\"subject\" required class=\"md-input\"&gt;\n\n        &lt;label for=\"message\" class=\"md-text\" style=\"text-align: left; padding-right: 10px;\"&gt;Message&lt;/label&gt;\n        &lt;textarea id=\"message\" name=\"message\" style=\"height: 200px;\" class=\"md-input\"&gt;&lt;/textarea&gt;\n\n        &lt;div&gt;&lt;/div&gt; &lt;!-- Empty cell for spacing --&gt;\n        &lt;button type=\"submit\" class=\"md-button\" style=\"width: fit-content; justify-self: start;\"&gt;Send&lt;/button&gt;\n    &lt;/form&gt;\n  &lt;/div&gt;\n&lt;/section&gt;\n</code></pre> <p>The form ID is the ID of the form that was created on the Un-static website. The form ID is used in the form's action attribute. The form ID is unique to each form.</p> <p>The form requires a return URL. This is the URL that the user is redirected to after the form is submitted. The return URL is specified when the form is created on the un-static website.</p> <p>This can just be a simple thank you page. Below is an example of a thank you page.</p> docs/contact/mailsent.md<pre><code>---\nhide:\n    - navigation\n    - toc\n---\nYour message has been sent\n</code></pre> <p>In order to not show the <code>mailsent</code> page in the navigation the navigation is hidden. I had set up the nav section in the <code>mkdocs.yml</code> like this:</p> mkdocs.yml<pre><code>nav:\n  - Home: index.md\n  - Blog:\n      ...\n  - Contact:\n      - Contact: contact/index.md\n      - Mail sent: contact/mailsent.md\n</code></pre>","tags":["website","un-static"]},{"location":"blog/2025/03/07/improved-shebang-for-your-python-standalone-script/","title":"Improved shebang for your python standalone script","text":"<p>Sometimes you just want a script, without having to specify a virtualenv or polluting your global python environment. Given PEP-0723 we can build scripts as we would normally, though now add a few comments to it to specify the dependencies.</p> <p>When we combine that with uv we can easily run the script without having to activate a virtualenv or specify the python interpreter. And it will just take care of it. Assume the script has the name <code>script.py</code> we would call it with <code>uv run -s script.py</code>.</p> <p>This is great on its own, it still is less ideal when you want to be able to call this script as part of your path. How can we fix this?</p> <p>Normally we would have a shebang in the python file pointing to a executable with a static path or using <code>env</code> to find the python interpreter.</p> <p>But wouldn't it be great if we could have a shebang that would take care of it and will have the <code>uv run</code> as part of it and you would even have to think about it anymore when trying to run the command from the terminal?</p>","tags":["uv","python","script"]},{"location":"blog/2025/03/07/improved-shebang-for-your-python-standalone-script/#the-fix","title":"The fix","text":"<p>Let's update the shebang and make it automatically use <code>uv run</code> to run the script. This way we can just call the script as if it was a normal executable.</p> <pre><code>#!/usr/bin/env -S uv run --script\n</code></pre> <p>The <code>-S</code> for <code>env</code> has the following in the man page: <pre><code>    -S string\n            Split apart the given string into multiple strings, and process each of the resulting strings as separate arguments\n            to the env utility.  The -S option recognizes some special character escape sequences and also supports\n            environment-variable substitution, as described below.\n</code></pre></p> <p>and the <code>--script</code> for <code>uv</code> has the following:</p> <pre><code>  -s, --script\n          Run the given path as a Python script.\n\n          Using `--script` will attempt to parse the path as a PEP 723 script, irrespective of its extension.\n</code></pre>","tags":["uv","python","script"]},{"location":"blog/2025/03/07/improved-shebang-for-your-python-standalone-script/#example","title":"Example","text":"<pre><code>#!/usr/bin/env -S uv run --script\n\n# /// script\n# requires-python = \"&gt;=3.12\"\n# dependencies = [\n#   \"click\",\n# ]\n# ///\n\nimport json\nfrom ipaddress import ip_address, ip_network\nfrom pathlib import Path\n\nimport click\n\n\n@click.command()\n@click.argument(\"peer_ip\")\ndef main(peer_ip: str) -&gt; None:\n    JSONFILE = Path(\"data.json\")\n    json_data = json.loads(JSONFILE.read_text())\n    ...\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["uv","python","script"]},{"location":"blog/2025/03/07/improved-shebang-for-your-python-standalone-script/#conclusion","title":"Conclusion","text":"<p>With this in place, we can just call the script from anywhere on the CLI and have <code>uv</code> take care of the dependencies and we will not have to provide the <code>uv run --script</code> ourselves.</p> <pre><code>/path/to/script.py\n</code></pre> <p>and its done.</p>","tags":["uv","python","script"]},{"location":"contact/","title":"\u2709\ufe0f Contact","text":"Name Email Subject Message Send"},{"location":"contact/mailsent/","title":"Mailsent","text":"<p>Your message has been sent</p>"},{"location":"speaker/","title":"\ud83c\udfa4 Public Speaking","text":""},{"location":"speaker/#public-speaking","title":"\ud83c\udfa4 Public speaking","text":""},{"location":"speaker/#presentations","title":"Presentations","text":""},{"location":"speaker/#bmp-going-beyond-show-commands-and-screen-scraping","title":"BMP - Going beyond show commands and screen scraping","text":"<ul> <li>Date: 2025-05-30</li> <li>Location: Autocon3</li> <li>Links:<ul> <li>Video</li> </ul> </li> </ul>"},{"location":"speaker/#description","title":"Description","text":"<p> With the automation around configuration generation and deployment already in place. It was the desire to present back to the customer that the BGP prefixes configured to be advertised were arrived as such. Though not only that, but also accepted through the policy.</p> <p>I will take you on the journey from where some felt that screen scraping was a good idea, to the current solution using BMP (BGP Monitoring Protocol). The chosen architecture, the challenges faced with the amount of data that comes with BGP and how we dealt with them.</p> <p>If you were looking on how to deal with your BGP data, especially your pre policy data, this should kickstart your adventure.</p>"},{"location":"speaker/#repos-are-like-children-parenting-101-pygrunn-2025","title":"Repos are like children, parenting 101 - PyGrunn 2025","text":"<ul> <li>Date: 2025-05-16</li> <li>Location: PyGrunn</li> <li>Links: Video</li> </ul>"},{"location":"speaker/#description_1","title":"Description","text":"<p> For those without children, you might never have realized that having a project is much like a child. At the same, for those with children, you may never have the time or piece of mind to realize it.</p> <p>I\u2019ll take you through a walk of life on \u2018creating\u2019 your child(ren), how to deal with the early stages, and taking care of the rules and boundaries as these youngsters mature. How do you ensure that you raise a child to be proud of?</p> <p>What happens when your child is going through different phases in life? Going from kindergarten, to school and beyond. How will he behave? Will he be overwhelmed by all the new info/data thrown at him?</p> <p>And so, you become a proud parent and have the brilliant idea (or someone had an idea for you) of having another child. Do you leave it to fate that he will mature the same, or could you influence it in any way?</p> <p>Would it be possible to provide our children the tools to take care of themselves?</p> <p>For whoever got confused, we are still talking about repositories and code. ;)</p>"},{"location":"speaker/#convert-poetry-to-uv-lightning","title":"Convert Poetry to uv - lightning","text":"<ul> <li>Date: 2025-03-27</li> <li>Location: Py.Amsterdam</li> <li>Links: Meetup</li> </ul>"},{"location":"speaker/#description_2","title":"Description","text":"<p> During the python meetup there was room for some 5 minute lightning talks. I stepped up and shared my findings on poetry and uv, and the challenging with migrating from poetry to uv. Because of this challenge I've written a tool to help with that migration. Allowing to pass in the current pyproject.toml and convert it to uv.</p> <p>The tool is hosted at github - convert_poetry2uv.</p>"},{"location":"speaker/#the-tale-of-2-henrys-and-bmp","title":"The Tale of 2 Henrys and BMP","text":"<p>What building cars can teach us about building software and BMP</p> <ul> <li>Date: 2025-03-06</li> <li>Location: DKNOG15</li> <li>Links: Video, Slides</li> </ul>"},{"location":"speaker/#description_3","title":"Description","text":"<p> In the late 19th century, two industrial titans were born within a few months of each other but an ocean apart. Both of these men, Henry Royce and Henry Ford, were obsessed with precision engineering and, fortunately for many of us, cars. They focused on building the best motor cars possible, though they achieved their goals in very different ways: Royce was driven by perfection, Ford by production.</p> <p>Michael Daly (Senior Director of Engineering) and Bart Dorlandt (Senior Network Automation Engineer) are working at Imperva where they are undergoing a complete rewrite of the Automation platform and we are using some of the ideas that these engineers have taught us us.</p> <p>This presentation will explore our vision for building better, more sustainable tools and discuss how the Network Automation team at Imperva is implementing these principles in our workflow.</p> <p>From the technical side we zoom in on a recent project using BGP Monitoring Protocol (BMP). Where we previously had manual network verification, which improved to screen scraping, now having a push model from the router to a central database. This gives us an \u201coffline\u201d state and allows the customer to self verify their configured and advertised prefixes are accepted and learned as expected. This didn\u2019t happen without challenges. We will share the pitfalls and the challenges and how we faced and overcame them.</p>"},{"location":"speaker/#repos-are-like-children-pyutrecht-2024","title":"Repos are like children - PyUtrecht 2024","text":"<p>parenting 101</p> <ul> <li>Date: 2024-09-17</li> <li>Location: PyUtrecht</li> <li>Links: Video - starting at 34:00, Slides</li> </ul>"},{"location":"speaker/#description_4","title":"Description","text":"<p> For those without children, you might never have realized that having a project is much like a child. At the same, for those with children, you may never have the time or piece of mind to realize it.</p> <p>I\u2019ll take you through a walk of life on \u2018creating\u2019 your child(ren), how to deal with the early stages, and taking care of the rules and boundaries as these youngsters mature. How do you ensure that you raise a child to be proud of?</p> <p>What happens when your child is going through different phases in life? Going from kindergarten, to school and beyond. How will he behave? Will he be overwhelmed by all the new info/data thrown at him?</p> <p>And so, you become a proud parent and have the brilliant idea (or someone had an idea for you) of having another child. Do you leave it to fate that he will mature the same, or could you influence it in any way?</p> <p>Would it be possible to provide our children the tools to take care of themselves?</p> <p>For whoever got confused, we are still talking about repositories and code. ;)</p> <p>During the talk we reference the growth of the child to several aspects of growing your repository and code. We therefore touch on the following subjects:</p> <ul> <li>poetry, rye, uv</li> <li>ruff</li> <li>cookiecutter</li> <li>pytest</li> <li>pipelines</li> <li>coverage, though use it wisely</li> </ul>"},{"location":"speaker/#podcasts","title":"Podcasts","text":""},{"location":"speaker/#there-must-be-a-better-way-network-automation-nerds","title":"There Must Be a Better Way! - Network Automation Nerds","text":"<ul> <li>Date: 2025-06-04</li> <li>Podcast: Packet Pushers - Network Automation Nerds</li> <li>Podcast links: Apple podcast, Spotify, Overcast, PocketCasts</li> </ul>"},{"location":"speaker/#description_5","title":"Description","text":"<p> \u201cThere must be a better way!\u201d is guest Bart Dorlandt\u2019s motto, which he applies to network automation, among other things. In today\u2019s episode, Bart shares what he\u2019s learned about network automation, explains why he focuses on process over tools, and reflects on the importance of mentorship. Bart and Eric also discuss why even if listeners aren\u2019t working on big automation projects, they can still look for better ways to manage their networks.</p> <ul> <li>Links mentioned in the podcast:<ul> <li>Convert Poetry to UV</li> <li>Convert Poetry to UV - PyPI</li> <li>Beyond the Makefile</li> <li>Raymond Hettinger Youtube videos</li> <li>Task</li> </ul> </li> </ul>"},{"location":"speaker/#deploying-the-bgp-monitoring-protocol-bmp-at-isp-scale","title":"Deploying the BGP Monitoring Protocol (BMP) at ISP Scale","text":"<ul> <li>Date: 2024-11-13</li> <li>Podcast: Packet Pushers - Heavy Networking</li> <li>Podcast links: Apple podcast, Spotify, Overcast, PocketCasts</li> </ul>"},{"location":"speaker/#description_6","title":"Description","text":"<p>The BGP Monitoring Protocol, or BMP, is an IETF standard. With BMP you can send BGP prefixes and updates from a router to a collector before any policy filters are applied. Once collected, you can analyze this routing data without any impact on the router itself. On today\u2019s Heavy Networking, we talk with Bart Dorlandt, a network automation solutions architect. An ISP approached Bart with a use case for BMP, and he designed and built a solution to serve the ISP\u2019s customers.</p> <p>We discuss what BMP is good for, how it works, and why Bart needed to use BMP. We also get into the tools he used to build his solution, the architecture he designed, the challenges he ran into in dealing with millions of records, why Kafka was essential for scaling, and more.</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/python/","title":"Python","text":""},{"location":"blog/category/ansible/","title":"Ansible","text":""},{"location":"blog/category/docker/","title":"Docker","text":""},{"location":"blog/category/taskfile/","title":"Taskfile","text":""},{"location":"blog/category/paperless/","title":"Paperless","text":""}]}
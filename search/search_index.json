{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#dream-networking-and-automation","title":"Dream networking and automation","text":"<p>At Dream networking and automation, we\u2019re passionate about making business connectivity effortless through smart automation and seamless networking. We know that complex networks can be overwhelming, so we focus on simplifying on the process that help businesses run more efficiently, and give them room to grow.\u00a0With innovative automation, we can help build reliable infrastructure that keep businesses running smoothly, minimize disruptions, and support new ideas in an ever-changing digital world.</p>"},{"location":"#what-can-we-do-for-you","title":"What can we do for you","text":"<p>Dream networking and automation is your trusted partner in transforming networking and automation challenges into strategic advantages. With years of experience in designing and implementing robust network automation solutions, we bring expertise, reliability, and a forward-thinking approach to every project. But it\u2019s not just about technical excellence, it\u2019s also about sharing knowledge, empowering your team, and fostering growth within your company. We take the time to understand your unique needs, ensuring that our solutions not only enhance efficiency and security but also strengthen your team\u2019s capabilities. By partnering with us, we are committed to delivering customized solutions that support both immediate success and long-term innovation. Let\u2019s build a smarter, more connected future together.</p> <p></p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"blog/tags/#tag:ansible","title":"ansible","text":"<ul> <li>            Managing dockers using ansible on Synology NAS          </li> </ul>"},{"location":"blog/tags/#tag:docker","title":"docker","text":"<ul> <li>            Taskfile magic with docker[-]compose          </li> </ul>"},{"location":"blog/tags/#tag:docker-compose","title":"docker-compose","text":"<ul> <li>            Managing dockers using ansible on Synology NAS          </li> <li>            Taskfile magic with docker[-]compose          </li> </ul>"},{"location":"blog/tags/#tag:email","title":"email","text":"<ul> <li>            Paperless          </li> </ul>"},{"location":"blog/tags/#tag:github-actions","title":"github-actions","text":"<ul> <li>            Pypi Trusted Publisher Management and pitfalls          </li> </ul>"},{"location":"blog/tags/#tag:oidc","title":"oidc","text":"<ul> <li>            Pypi Trusted Publisher Management and pitfalls          </li> </ul>"},{"location":"blog/tags/#tag:paperless","title":"paperless","text":"<ul> <li>            Paperless          </li> </ul>"},{"location":"blog/tags/#tag:poetry","title":"poetry","text":"<ul> <li>            Convert poetry to uv          </li> </ul>"},{"location":"blog/tags/#tag:pypi","title":"pypi","text":"<ul> <li>            Pypi Trusted Publisher Management and pitfalls          </li> </ul>"},{"location":"blog/tags/#tag:python","title":"python","text":"<ul> <li>            Convert poetry to uv          </li> <li>            Improved shebang for your python standalone script          </li> <li>            Pypi Trusted Publisher Management and pitfalls          </li> </ul>"},{"location":"blog/tags/#tag:script","title":"script","text":"<ul> <li>            Improved shebang for your python standalone script          </li> </ul>"},{"location":"blog/tags/#tag:services","title":"services","text":"<ul> <li>            Contact form made simple          </li> </ul>"},{"location":"blog/tags/#tag:shortcuts","title":"shortcuts","text":"<ul> <li>            Paperless          </li> </ul>"},{"location":"blog/tags/#tag:synology","title":"synology","text":"<ul> <li>            Managing dockers using ansible on Synology NAS          </li> </ul>"},{"location":"blog/tags/#tag:taskfile","title":"taskfile","text":"<ul> <li>            Taskfile magic with docker[-]compose          </li> </ul>"},{"location":"blog/tags/#tag:un-static","title":"un-static","text":"<ul> <li>            Contact form made simple          </li> </ul>"},{"location":"blog/tags/#tag:uv","title":"uv","text":"<ul> <li>            Convert poetry to uv          </li> <li>            Improved shebang for your python standalone script          </li> </ul>"},{"location":"blog/2025/05/27/convert-poetry-to-uv/","title":"Convert poetry to uv","text":"<p> When I encountered #uv I fell for it right away. I was already using ruff for a while and started looking into rye, which was the intermediate step before it became uv.</p> <p>Creating a uv repo is easy, just run <code>uv init $project</code> and you are off to the races. Using it for new projects is easy if you are already used to poetry. The commands are quite similar. Just add a few libraries with <code>uv add $library</code> or <code>uv add --dev $library</code> to make it part of the development group.</p> <p>Even loading libraries from your requirements file is easy, <code>uv add -r requirements.txt</code>.</p> <p>Now, you might want to migrate other repositories to have some consistency.  How to migrate your existing repo's to uv. Doing it by hand is just plain annoying and it will be easy to sneak in some errors.</p> <p>After I did that once, and imaging other would be in the same pickle. I thought it would be useful to have a migration script to convert the existing #poetry <code>pyproject.toml</code> to one that can be consumed by #uv.</p> <p>I looked for existing ones, but didn't find any that met my requirements or desires. So, nothing to do than creating my own. And so it was born. convert_poetry2uv</p> <p>to make it really easy to use, when you already have uv installed is just calling it directly:</p> <pre><code>uvx convert-poetry2uv -n pyproject.toml\n</code></pre> <p>The <code>-n</code> flag will do a dry run and creates a new file instead of moving the old one out of the way, allowing you to verify its contents. To do the actual migration, just:</p> <pre><code>uvx convert-poetry2uv pyproject.toml\n</code></pre> <p>(Your original file will be backed up)</p> <p>I hope this project will help you in your migrations and make them smooth and quick.</p> <p>In case you run into challenges, don't hesitate to reach out.</p> <p>Share on </p>","tags":["python","poetry","uv"]},{"location":"blog/2025/06/13/taskfile-magic-with-docker-compose/","title":"Taskfile magic with docker[-]compose","text":"<p>I have been using Taskfile for a while now, and I really like it. It is a great alternative to Makefile, and it has some nice features that make it easier to use.</p> <p>In this post I wanted to share some \"magic\" to be able to select the right docker compose application to use. Either the older <code>docker-compose</code> or the newer <code>docker compose</code> command, using the compose plugin.</p> <p>I have a <code>Taskfile.yml</code> that looks like this that allows me to update running dockers, by finding them using <code>docker compose ls</code> and with those entries follow the path and pull the images and restart the containers.</p> <p>I want to be able to use the same tasks, even though I have to work with a system that still uses the older <code>docker-compose</code> command.</p>","tags":["taskfile","docker-compose","docker"]},{"location":"blog/2025/06/13/taskfile-magic-with-docker-compose/#how-to-deal-with-this","title":"How to deal with this?","text":"<p>In the <code>var</code> section of the <code>Taskfile.yml</code>, I define a variable <code>DOCKERCOMPOSE</code> that will be set to either <code>docker compose</code> or <code>docker-compose</code> depending on the availability of the command.</p> <p>The command is: <code>command -v docker-compose &amp;&gt; /dev/null &amp;&amp; echo \"docker-compose\" || echo \"docker compose\"</code></p> <p>The section would therefore result in:</p> <pre><code>vars:\n  DOCKERCOMPOSE:\n    sh: command -v docker-compose &amp;&gt; /dev/null &amp;&amp; echo \"docker-compose\" || echo \"docker compose\"\n</code></pre>","tags":["taskfile","docker-compose","docker"]},{"location":"blog/2025/06/13/taskfile-magic-with-docker-compose/#taskfile-usage","title":"Taskfile usage","text":"<p>This will allow you to use the <code>DOCKERCOMPOSE</code> variable in your tasks, like so:</p> <pre><code>tasks:\n  up:\n    dir: \"{{.USER_WORKING_DIR}}\"\n    preconditions:\n      - test -f docker-compose.yml\n    cmds:\n      - \"{{.SUDO}} {{.DOCKERCOMPOSE}} up -d --remove-orphans\"\n</code></pre> <p>I have the same kind of logic around <code>{{.SUDO}}</code>. If this environment variable is set, it will use <code>sudo</code> to run the command, otherwise it will just run the command without <code>sudo</code>. This therefore depends on the system I used it on.</p> <p>Essentially, this allows me to use the same <code>Taskfile.yml</code> on different systems, without having to change the commands or the tasks.</p>","tags":["taskfile","docker-compose","docker"]},{"location":"blog/2025/06/13/taskfile-magic-with-docker-compose/#more-tasks-around-docker-compose","title":"More tasks around docker compose","text":"<p>Below some more tasks that I use in my <code>Taskfile.yml</code>:</p> <pre><code>version: \"3\"\n\n# env SUDO=sudo may be set and will be used if so\n\nvars:\n  DOCKERCOMPOSE: \"docker compose\"\n\ntasks:\n  default:\n    cmd: task -l --sort none\n    silent: true\n\n  ### Per project tasks\n  up:\n    desc: \"Start the docker-compose environment\"\n    dir: \"{{.USER_WORKING_DIR}}\"\n    preconditions:\n      - test -f docker-compose.yml\n    cmds:\n      - \"{{.SUDO}} {{.DOCKERCOMPOSE}} up -d --remove-orphans\"\n\n  down:\n    desc: \"Stop the docker-compose environment\"\n    dir: \"{{.USER_WORKING_DIR}}\"\n    preconditions:\n      - test -f {{.USER_WORKING_DIR}}/docker-compose.yml\n    cmds:\n      - \"{{.SUDO}} {{.DOCKERCOMPOSE}} down\"\n\n  ps:\n    desc: \"List the containers in the docker-compose environment\"\n    dir: \"{{.USER_WORKING_DIR}}\"\n    preconditions:\n      - test -f {{.USER_WORKING_DIR}}/docker-compose.yml\n    cmds:\n      - \"{{.SUDO}} {{.DOCKERCOMPOSE}} ps\"\n\n  update:\n    desc: \"Update the docker-compose environment\"\n    dir: \"{{.USER_WORKING_DIR}}\"\n    preconditions:\n      - test -f {{.USER_WORKING_DIR}}/docker-compose.yml\n    cmds:\n      - \"{{.SUDO}} {{.DOCKERCOMPOSE}} pull\"\n\n  restart:\n    desc: \"Restart the docker-compose environment\"\n    dir: \"{{.USER_WORKING_DIR}}\"\n    preconditions:\n      - test -f {{.USER_WORKING_DIR}}/docker-compose.yml\n    cmds:\n      - \"{{.SUDO}} {{.DOCKERCOMPOSE}} restart\"\n\n  ### Global tasks\n  update-all:\n    desc: \"Update all docker-compose environments\"\n    aliases:\n      - update_all\n    preconditions:\n      - \"[ `uname -n` = nas ]\"\n    cmd: |\n      this_dir=$(pwd)\n      echo \"Get running {{.DOCKERCOMPOSE}} envs\"\n      running=$({{.SUDO}} {{.DOCKERCOMPOSE}} ls | grep running | awk '{print $1}')\n      for x in $running; do\n        cd $x\n        pwd\n        task update\n        task up\n        cd ${this_dir}\n        echo \"\"\n      done\n\n  restart-all:\n    desc: \"Restart all docker-compose environments\"\n    aliases:\n      - restart_all\n    preconditions:\n      - \"[ `uname -n` = nas ]\"\n    cmd: |\n      this_dir=$(pwd)\n      echo \"Get running {{.DOCKERCOMPOSE}} envs\"\n      running=$({{.SUDO}} {{.DOCKERCOMPOSE}} ls | grep running | awk '{print $1}')\n      for x in $running; do\n        cd $x\n        pwd\n        {{.SUDO}} {{.DOCKERCOMPOSE}} restart\n        cd ${this_dir}\n        echo \"\"\n      done\n\n  prune:images:\n    desc: \"Prune unused docker images\"\n    interactive: true\n    cmds:\n      - \"{{.SUDO}} docker image prune\"\n</code></pre> <p>Share on </p>","tags":["taskfile","docker-compose","docker"]},{"location":"blog/2025/03/02/paperless/","title":"Paperless","text":"<p>Recently I was helping out some friends with paperless, how to structure their documents, how to implement a flow for their documents, etc. On occasion this triggered me to look at how I use paperless myself. I've been using it for a while now and I'm very happy with it. I've also added some automation to it, which I'll share in this post.</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#the-start","title":"The start","text":"<p>Getting started with paperless is quite easy. The documentation is good and will get you started quickly. I'm running it in a docker container, which is also described in the documentation. I'm running the <code>tika</code> variant with <code>postgres</code>. See the docker-compose.yml for the details.</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#beautiful-part","title":"Beautiful part","text":"<p>Let's skip right to the beautiful part. No matter how you start your setup with paperless, there is no wrong way. No matter how you start with tags, correspondents, document types or storage paths, you can always change them later. This is what I love about paperless. You can start with a simple setup and grow it as you go.</p> <p>Even the storage path structure, which is already very flexible in paperless, can be changed later. Any change to it, will be reflected on the filesystem without any worries on failing.</p> <p>Here are some examples of how I've structured my documents:</p> <p>Contracts: <pre><code>contracts/{{ created_year }}/{{ correspondent }}/{{ created_year }}{{ created_month }}{{ created_day }}_{{ title }}\n</code></pre></p> <p>Invoices private: <pre><code>private/invoices/{{ created_year }}/{{ correspondent }}/{{ created_year }}{{ created_month }}{{ created_day }}_{{ title }}\n</code></pre></p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#algorithms","title":"Algorithms","text":"<p>For all \"things\" in paperless, you can choose to have it done \"automagically\" or helping it out a little bit.</p> <p>As with the storage paths, the matching can be done using it algorithms, these will start working after learning about a few (if I recall correctly, ~ 20 documents).</p> <p>Otherwise, you can have it match on the presence of certain words, either exact or via regex, and some more options. I'd say, there is no best way, just your way of choosing how to use it.</p> <p>The same logic applies to <code>tags</code>, <code>document types</code> and <code>correspondents</code>.</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#automation","title":"Automation","text":"<p>I'll go into some of the internal automation as well as some external ones.</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#custom-fields-and-workflows","title":"Custom fields and workflows","text":"<p>For the invoices I've created a custom field <code>Total</code>. This is a <code>Monetary</code> field. I want this field to be present on any invoice.</p> <p>This can be done manually, but where is the fun in that?</p> <p>I've created a workflow that add the <code>total</code> field to the respective document type automatically. This is done by a trigger when a documented is added to the <code>invoices</code> category or when it is updated to it.</p> <p></p> <p>See an example below, note that <code>Factuur</code> is Dutch for <code>Invoice</code>.</p> <p></p> <p>In the \"Actions\" section you can assign the custom field <code>Total</code> to the document.</p> <p></p> <p>Note: What this doesn't do: When it was incorrectly assigned to invoices, it will not remove the custom field. Though this is still easily done when processing your inbox.</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#email","title":"Email","text":"<p>I have set up a dedicated mailbox for paperless to monitor. Every hour paperless scan the mailbox for new mails. It will process the documents and add them to the database.</p> <p>With this in place, I can just send/forward any mail. Quite useful! I didn't want to give it access to my private or work mailboxes, that is why I've setup up a dedicated mailbox.</p> <p>Having the dedicated mailbox I can have some additional automation for it. When invoices are received to a specific alias, they will also be forwarded to my bookkeeping program, allowing me to process that email in both systems with a single action.</p> <p>Therefore, depending on the alias, the document will be processed differently.</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#paperless-app-the-phone","title":"Paperless app the phone","text":"<p>I've also set up the paperless app on my phone. This allows me to quickly scan documents and have them processed by paperless. I can also add tags, correspondents and categories to the document. This is quite useful when I'm on the go and want to process a document quickly.</p> <p>Sharing documents from the browser is not really a quick option, you would need to download the file and take an action on it.</p> <p>With the application you can look at a document and quickly take an action on it. Including having your own shortcuts..</p> <p>(I'm sure there is an app on Android as well)</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#shortcuts","title":"Shortcuts","text":"<p>I've created some shortcuts on my phone to quickly add a document to paperless or to email them to the different mail addresses for the various actions.</p> <p>With these in place I'm able to quickly process documents, from the phone, tablet or laptop.</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#add-document-to-paperless-with-the-api","title":"Add document to paperless with the API","text":"<p>Here is a public shortcut to add a file/pdf/image to paperless using the api. On the first run you will need to set the API key and the URL to your paperless instance.</p> <p>This might be less useful on the phone, where you have the app, but it might be useful on the tablet or laptop.</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#email-documents-with-quick-actions","title":"Email documents with quick actions","text":"<p>Here is my shortcut for sending a file to an email list, where you can select which predefined email address it goes to.</p> <p>This allows me to use the quickactions on the phone, tablet or laptop to send an email address of choice.</p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/03/02/paperless/#missing-features","title":"Missing features?","text":"<p>With all this greatness in place, what could possible be missing?</p> <p>Having a forward/send button or something like a quick action available within paperless would be awesome to have. This would allow me to quickly forward a document to my bookkeeping program as an example.</p> <p>Though, even without this feature, I'm very happy with paperless and the automation I've added to it. Share on </p>","tags":["paperless","shortcuts","email"]},{"location":"blog/2025/01/07/pypi-trusted-publisher-management-and-pitfalls/","title":"Pypi Trusted Publisher Management and pitfalls","text":"<p> Be the cool kid on pypi, I thought, use the Trusted Publisher Management and OpenID Connect (OIDC)... I thought...</p> <p>While working on my latest repo convert_poetry2uv, I wanted to automatically push the builds to pypi. Traditionally a username/password combination was used, which was later replaced by an API token. These days OIDC can be used, which I tried. I'm here to share some pitfalls, so hopefully you don't fall in them.</p>","tags":["pypi","python","oidc","github-actions"]},{"location":"blog/2025/01/07/pypi-trusted-publisher-management-and-pitfalls/#the-beginning","title":"The beginning","text":"<p>Let's start with the guides I followed:</p> <p>https://packaging.python.org/en/latest/guides/publishing-package-distribution-releases-using-github-actions-ci-cd-workflows/</p> <p>This is actually quite clear and will get you started quickly, with also good examples. I only changed the actions yaml a bit, to work with uv.</p> <p>Another doc is by pypi, https://docs.pypi.org/trusted-publishers/creating-a-project-through-oidc/</p> <p>So, let's start with which field needs to be what.</p> <p>This is what is currently being used for my test flow, to push to test.pypi.org.</p> <p></p> <p>Create a new publisher</p>","tags":["pypi","python","oidc","github-actions"]},{"location":"blog/2025/01/07/pypi-trusted-publisher-management-and-pitfalls/#any-caveats","title":"Any caveats?","text":"<p>So what are the caveats, this looks simple enough, right?</p>","tags":["pypi","python","oidc","github-actions"]},{"location":"blog/2025/01/07/pypi-trusted-publisher-management-and-pitfalls/#typos","title":"Typos","text":"<p>obviously!</p>","tags":["pypi","python","oidc","github-actions"]},{"location":"blog/2025/01/07/pypi-trusted-publisher-management-and-pitfalls/#project-name","title":"Project name","text":"<p>The next thing is the PyPi Project Name. This needs to match the name defined in your <code>pyproject.toml</code> (no underscore, but hyphens!). If it doesn't match you'll get 403 errors when trying to upload.</p> 403 error<pre><code>  &lt;title&gt;403 Invalid API Token: OIDC scoped token is not valid for\n    project 'poetry-to-uv', project-scoped token is not valid for project:\n    'poetry-to-uv'&lt;/title&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n      &lt;h1&gt;403 Invalid API Token: OIDC scoped token is not valid for project\n    'poetry-to-uv', project-scoped token is not valid for project:\n    'poetry-to-uv'&lt;/h1&gt;\n      Access was denied to this resource.&lt;br/&gt;&lt;br/&gt;\n    Invalid API Token: OIDC scoped token is not valid for project\n    &amp;#x27;poetry-to-uv&amp;#x27;, project-scoped token is not valid for\n    project: &amp;#x27;poetry-to-uv&amp;#x27;\n</code></pre> <p>NOTE: this error still has the old project name, which was part of my original challenge</p>","tags":["pypi","python","oidc","github-actions"]},{"location":"blog/2025/01/07/pypi-trusted-publisher-management-and-pitfalls/#-vs-_","title":"\"-\" vs \"_\"","text":"<p>During my initial steps I also ran into a invalid-publisher. This is described on the troubleshooting page.</p> <p>In my case, this was a mismatch between \"_\" and \"-\".</p>","tags":["pypi","python","oidc","github-actions"]},{"location":"blog/2025/01/07/pypi-trusted-publisher-management-and-pitfalls/#not-a-problem","title":"Not a problem","text":"<p>Having a repository name different than the project name is not a problem, as long as they are specified correctly on the pypi.org publishing page.</p>","tags":["pypi","python","oidc","github-actions"]},{"location":"blog/2025/01/07/pypi-trusted-publisher-management-and-pitfalls/#example-highlights","title":"Example highlights","text":"<p>At the bottom of this article is my complete <code>publish-to-pypi.yml</code> workflow.</p> <p>Here are some highlight on the snippets.</p> environment<pre><code> publish-to-testpypi:\n    name: Upload release to TestPyPI\n    needs:\n      - build\n    runs-on: ubuntu-latest\n\n    environment:\n      name: testpypi\n      url: https://test.pypi.org/p/convert-poetry2uv\n</code></pre> <p>Note the name of the environment, it matches the environment that is configured in the publisher on pypi.org.</p> repository-url<pre><code> - name: Publish distribution \ud83d\udce6 to TestPyPI\n        uses: pypa/gh-action-pypi-publish@release/v1\n        with:\n          repository-url: https://test.pypi.org/legacy/\n          verbose: true\n          skip-existing: true\n</code></pre> <p>For TestPyPi the repository-url needs to be set. I've set the verbose to true, so I was able to inspect my issues. The skip-existing flag is useful for testing on TestPyPi. It will not overwrite the existing file with the same version number, but it will not fail the pipeline either.</p>","tags":["pypi","python","oidc","github-actions"]},{"location":"blog/2025/01/07/pypi-trusted-publisher-management-and-pitfalls/#full-example","title":"Full example","text":"<p>For the full section below.</p> <ul> <li>Only push to pypi when a new tag is set</li> </ul> publish-to-pypi<pre><code>  publish-to-pypi:\n    name: Publish Python \ud83d\udc0d distribution \ud83d\udce6 to PyPI\n    if: startsWith(github.ref, 'refs/tags/') # only publish to PyPI on tag pushes\n</code></pre> <ul> <li>${{ github.token }}, is automatically set. No need to worry about it.</li> </ul> <pre><code> - name: Create GitHub Release\n    env:\n      GITHUB_TOKEN: ${{ github.token }}\n</code></pre> <ul> <li>With the upload to pypi successful, a release is created also on github.</li> </ul> publish-to-pypi.yml<pre><code>name: Publish to TestPyPI\n\non: push\n\njobs:\n  build:\n    name: Build distribution \ud83d\udce6\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v5\n        with:\n          # Install a specific version of uv.\n          version: \"0.5.13\"\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.12\"\n\n      - name: build distribution\n        run: uv build\n\n      - name: Store the distribution packages\n        uses: actions/upload-artifact@v4\n        with:\n          name: python-package-distributions\n          path: dist/\n\n  publish-to-testpypi:\n    name: Upload release to TestPyPI\n    needs:\n      - build\n    runs-on: ubuntu-latest\n\n    environment:\n      name: testpypi # (1)!\n      url: https://test.pypi.org/p/convert-poetry2uv\n\n    permissions:\n      id-token: write # IMPORTANT: mandatory for trusted publishing\n\n    steps:\n      - name: Download all the dists\n        uses: actions/download-artifact@v4\n        with:\n          name: python-package-distributions\n          path: dist/\n      - name: Publish distribution \ud83d\udce6 to TestPyPI\n        uses: pypa/gh-action-pypi-publish@release/v1\n        with:\n          repository-url: https://test.pypi.org/legacy/\n          verbose: true\n          skip-existing: true\n\n  publish-to-pypi:\n    name: Publish Python \ud83d\udc0d distribution \ud83d\udce6 to PyPI\n    if: startsWith(github.ref, 'refs/tags/') # (3)! only publish to PyPI on tag pushes\n    needs:\n      - build\n    runs-on: ubuntu-latest\n\n    environment:\n      name: pypi # (2)!\n      url: https://pypi.org/p/convert-poetry2uv\n\n    permissions:\n      id-token: write # IMPORTANT: mandatory for trusted publishing\n\n    steps:\n      - name: Download all the dists\n        uses: actions/download-artifact@v4\n        with:\n          name: python-package-distributions\n          path: dist/\n      - name: Publish distribution \ud83d\udce6 to PyPI\n        uses: pypa/gh-action-pypi-publish@release/v1\n\n  github-release:\n    name: &gt;-\n      Sign the Python \ud83d\udc0d distribution \ud83d\udce6 with Sigstore\n      and upload them to GitHub Release\n    needs:\n      - publish-to-pypi\n    runs-on: ubuntu-latest\n\n    permissions:\n      contents: write # IMPORTANT: mandatory for making GitHub Releases\n      id-token: write # IMPORTANT: mandatory for sigstore\n\n    steps:\n      - name: Download all the dists\n        uses: actions/download-artifact@v4\n        with:\n          name: python-package-distributions\n          path: dist/\n      - name: Sign the dists with Sigstore\n        uses: sigstore/gh-action-sigstore-python@v3.0.0\n        with:\n          inputs: &gt;-\n            ./dist/*.tar.gz\n            ./dist/*.whl\n      - name: Create GitHub Release\n        env:\n          GITHUB_TOKEN: ${{ github.token }} # (4)!\n        run: &gt;-\n          gh release create\n          \"$GITHUB_REF_NAME\"\n          --repo \"$GITHUB_REPOSITORY\"\n          --notes \"\"\n      - name: Upload artifact signatures to GitHub Release # (5)!\n        env:\n          GITHUB_TOKEN: ${{ github.token }}\n        # Upload to GitHub Release using the `gh` CLI.\n        # `dist/` contains the built packages, and the\n        # sigstore-produced signatures and certificates.\n        run: &gt;-\n          gh release upload\n          \"$GITHUB_REF_NAME\" dist/**\n          --repo \"$GITHUB_REPOSITORY\"\n</code></pre> <ol> <li>Note the name of the environment, it matches the environment that is configured in the publisher on test.pypi.org.</li> <li>Note the name of the environment, it matches the environment that is configured in the publisher on pypi.org.</li> <li>Only push to pypi when a new tag is set</li> <li>${{ github.token }}, is automatically set. No need to worry about it.</li> <li>With the upload to pypi successful, a release is created also on github</li> </ol> <p>Have fun.</p> <p>Share on </p>","tags":["pypi","python","oidc","github-actions"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/","title":"Managing dockers using ansible on Synology NAS","text":"<p>Synology NAS devices are great and easy to use devices. The model I use is plenty strong to running a few containers as well. However, the Synology Docker application is not the best when it comes to managing multiple containers. It is a bit clunky and does not provide the flexibility I want.</p> <p>Managing containers is very well possible using Ansible. Especially when you already use it for different systems and different tasks. Adding a playbook to allow you to update or manage your containers is a no-brainer.</p> <p>Let's start with the setup, and leave synology aside for a moment.</p> <p>I wanted to extend my playbook for updating my servers, including the containers running on them.</p>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#the-playbook","title":"The playbook","text":"<p>I have added the following part to my playbook:</p> <pre><code>- name: Update docker compose\n  hosts: dockerCompose\n  become: true\n  roles:\n    - { role: docker_compose, tags: docker_compose }\n</code></pre> <p>This will run the <code>docker_compose</code> role on the <code>dockerCompose</code> hosts. The <code>become: true</code> allows the playbook to run with elevated privileges, which is required to manage docker containers.</p>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#the-hosts","title":"The hosts","text":"<p>The hosts are defined in my <code>hosts.yaml</code> file:</p> <pre><code>dockerCompose:\n  hosts:\n    hostname1:\n    hostname2:\n</code></pre>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#the-role-docker_compose","title":"The role <code>docker_compose</code>","text":"<p>The role <code>docker_compose</code> is defined in my Ansible roles directory. The role contains the tasks to manage the docker containers, using the docker_compose_v2 module. The <code>roles/docker_compose/tasks/main.yml</code> looks like this:</p> <pre><code>---\n- name: Update dockers\n  become: true\n  community.docker.docker_compose_v2:\n    docker_cli: \"{{ docker_cli | default(omit) }}\"\n    project_src: \"{{ item.path }}\"\n    remove_orphans: true\n    state: restarted\n  loop: \"{{ dockers }}\"\n</code></pre> <p>This tasks will loop over the <code>dockers</code> variable, which is defined in the <code>hostvars</code>. This will have the paths to the docker compose files that need to be managed.</p> <pre><code>dockers:\n  - path: /path/to/project/holding/docker-compose_file\n  - path: /path/to/project1/holding/docker-compose_file\n  - path: /path/to/project2/holding/docker-compose_file\n</code></pre> <p>The <code>state: restarted</code> will ensure that the containers are restarted after pulling the latest images. The <code>remove_orphans: true</code> will remove any containers that are no longer defined in the docker compose file.</p> <p>The <code>docker_cli</code> variable is optional and can be used to specify the docker CLI command to use. If not set, it will use the default <code>docker compose</code> command. For my synology I had to specify it.</p> <p>Specify the <code>docker_cli</code> variable in the <code>host_vars</code> file for the Synology NAS:</p> <pre><code>docker_cli: /usr/local/bin/docker\n</code></pre>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#docker-compose-plugin","title":"Docker Compose Plugin","text":"<p>In the past, I had to do certain modifications to the synology python environment to be able to achieve this. Now I have modified the setup to use the compose plugin. Especially, that since 2022 the <code>docker-compose</code> command is deprecated and replaced by the <code>docker compose</code> command. The <code>community.docker.docker_compose_v2</code> module will use the newer <code>docker compose</code> command, based on the plugin.</p> <p>This plugin needs to be installed on the Synology NAS. I have created the following task to install the plugin.</p> <p>In the same role, <code>roles/docker_compose/tasks/synology.yml</code>:</p> <pre><code>---\n- name: Create a docker plugins directory\n  ansible.builtin.file:\n    path: \"{{ docker_plugins_path }}\"\n    state: directory\n    mode: \"0755\"\n\n- name: Download docker compose plugin\n  ansible.builtin.get_url:\n    url: https://github.com/docker/compose/releases/download/{{ docker_compose_version }}/docker-compose-linux-x86_64\n    dest: \"{{ docker_plugins_path }}/docker-compose\"\n    mode: \"0755\"\n</code></pre> <p>The following are added to the <code>hostvars</code>:</p> <pre><code>docker_compose_version: v2.37.1\ndocker_plugins_path: /usr/local/lib/docker/cli-plugins\n</code></pre>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#playbook-addition","title":"Playbook addition","text":"<p>To call this task, we add the following to the playbook, before calling the <code>docker_compose</code> role:</p> <pre><code>- name: Docker compose for Synology hosts\n  hosts: synology\n  become: true\n  tasks:\n    - name: Include Synology tasks\n      ansible.builtin.include_tasks: roles/docker_compose/tasks/synology.yml\n</code></pre>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#conclusion","title":"Conclusion","text":"<p>Using Ansible to manage docker containers on a Synology NAS is a great way to automate the management of your containers. It allows you to easily update and manage your containers, without having to use the Synology Docker application. The <code>community.docker.docker_compose_v2</code> module provides a simple way to manage your containers using the newer <code>docker compose</code> command, which is the recommended way to manage docker containers.</p>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#complete-files","title":"Complete files","text":"","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#update_serversyaml-playbook","title":"update_servers.yaml playbook","text":"<pre><code>---\n- name: Docker compose for Synology hosts\n  hosts: synology\n  become: true\n  tasks:\n    - name: Include Synology tasks\n      ansible.builtin.include_tasks: roles/docker_compose/tasks/synology.yml\n\n- name: Update docker compose\n  hosts: dockerCompose\n  become: true\n  roles:\n    - { role: docker_compose, tags: docker_compose }\n</code></pre>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#rolesdocker_composetasksmainyml","title":"roles/docker_compose/tasks/main.yml","text":"<pre><code>---\n- name: Install dependencies\n  ansible.builtin.command:\n    cmd: \"{{ item.install_command }}\"\n    chdir: \"{{ item.path }}\"\n  loop: \"{{ dockers }}\"\n  when: item.install_command is defined\n  register: my_output\n  changed_when: my_output.rc != 0 # &lt;- Uses the return code to define when the task has changed.\n  become_user: \"{{ user.name }}\"\n  become: true\n\n- name: Update dockers\n  become: true\n  community.docker.docker_compose_v2:\n    docker_cli: \"{{ docker_cli | default(omit) }}\"\n    project_src: \"{{ item.path }}\"\n    remove_orphans: true\n    state: present\n    ignore_build_events: false\n  loop: \"{{ dockers }}\"\n</code></pre>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#rolesdocker_composetaskssynologyyml","title":"roles/docker_compose/tasks/synology.yml","text":"<pre><code>---\n- name: Create a docker plugins directory\n  ansible.builtin.file:\n    path: \"{{ docker_plugins_path }}\"\n    state: directory\n    mode: \"0755\"\n\n- name: Download docker compose plugin\n  ansible.builtin.get_url:\n    url: https://github.com/docker/compose/releases/download/{{ docker_compose_version }}/docker-compose-linux-x86_64\n    dest: \"{{ docker_plugins_path }}/docker-compose\"\n    mode: \"0755\"\n</code></pre>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#host_varshostname1mainyaml","title":"host_vars/hostname1/main.yaml","text":"<pre><code>dockers:\n  - path: /path/to/project/holding/docker-compose_file\n  - path: /path/to/project1/holding/docker-compose_file\n  - path: /path/to/project2/holding/docker-compose_file\n\ndocker_cli: /usr/local/bin/docker\ndocker_compose_version: v2.37.1\ndocker_plugins_path: /usr/local/lib/docker/cli-plugins\n</code></pre>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/06/14/managing-dockers-using-ansible-on-synology-nas/#hostsyaml","title":"hosts.yaml","text":"<pre><code>dockerCompose:\n  hosts:\n    hostname1:\n    # hostname2:\n\nsynology:\n  hosts:\n    hostname1:\n      ansible_host: 192.168.3.20\n</code></pre> <p>Share on </p>","tags":["ansible","docker-compose","synology"]},{"location":"blog/2025/01/06/contact-form-made-simple/","title":"Contact form made simple","text":"<p> Today I ran into a service that allows you to create a contact form without having to write a bunch of code. Making it really easy to incorporate it in your mkdocs website.</p>","tags":["services","un-static"]},{"location":"blog/2025/01/06/contact-form-made-simple/#un-static-forms","title":"Un-static forms","text":"<p>The service is called Un-static and it allows you to create a form by registering a form on their website. Essentially, you don't even need to create an account unless you need some additional features.</p> <p>Once you create a form you can use the form's URL to submit the form. The form is submitted via a POST request and the form data is sent to the email address you specified when  the form was created.</p> <p>The how-to page on their website explains how to create a static form and how to use it.</p>","tags":["services","un-static"]},{"location":"blog/2025/01/06/contact-form-made-simple/#example","title":"Example","text":"<p>Below is an example of a form that I created using Un-static in combination with Mkdocs.</p> docs/contact/index.md<pre><code>---\nhide:\n    - navigation\n    - toc\n---\n&lt;section&gt;\n  &lt;div class=\"md-grid\"\n  style=\"max-width: 840px; margin-left: 0; display: flex; justify-content: left;\"&gt;\n    &lt;form method=\"post\"\n    action=\"https://forms.un-static.com/forms/&lt;form-id&gt;\"\n    class=\"md-grid md-form\"\n    style=\"display: grid; grid-template-columns: 150px 2fr; gap: 10px; width: 100%;\"&gt;\n        &lt;label for=\"name\" class=\"md-text\" style=\"text-align: left; padding-right: 10px;\"&gt;Name&lt;/label&gt;\n        &lt;input type=\"text\" id=\"name\" name=\"name\" required class=\"md-input\"&gt;\n\n        &lt;label for=\"email\" class=\"md-text\" style=\"text-align: left; padding-right: 10px;\"&gt;Email&lt;/label&gt;\n        &lt;input type=\"email\" id=\"email\" name=\"email\" required class=\"md-input\"&gt;\n\n        &lt;label for=\"subject\" class=\"md-text\" style=\"text-align: left; padding-right: 10px;\"&gt;Subject&lt;/label&gt;\n        &lt;input type=\"text\" id=\"subject\" name=\"subject\" required class=\"md-input\"&gt;\n\n        &lt;label for=\"message\" class=\"md-text\" style=\"text-align: left; padding-right: 10px;\"&gt;Message&lt;/label&gt;\n        &lt;textarea id=\"message\" name=\"message\" style=\"height: 200px;\" class=\"md-input\"&gt;&lt;/textarea&gt;\n\n        &lt;div&gt;&lt;/div&gt; &lt;!-- Empty cell for spacing --&gt;\n        &lt;button type=\"submit\" class=\"md-button\" style=\"width: fit-content; justify-self: start;\"&gt;Send&lt;/button&gt;\n    &lt;/form&gt;\n  &lt;/div&gt;\n&lt;/section&gt;\n</code></pre> <p>The form ID is the ID of the form that was created on the Un-static website. The form ID is used in the form's action attribute. The form ID is unique to each form.</p> <p>The form requires a return URL. This is the URL that the user is redirected to after the form is submitted. The return URL is specified when the form is created on the un-static website.</p> <p>This can just be a simple thank you page. Below is an example of a thank you page.</p> docs/contact/mailsent.md<pre><code>---\nhide:\n    - navigation\n    - toc\n---\nYour message has been sent\n</code></pre> <p>In order to not show the <code>mailsent</code> page in the navigation the navigation is hidden. I had set up the nav section in the <code>mkdocs.yml</code> like this:</p> <p>mkdocs.yml<pre><code>nav:\n  - Home: index.md\n  - Blog:\n      ...\n  - Contact:\n      - Contact: contact/index.md\n      - Mail sent: contact/mailsent.md\n</code></pre> Share on </p>","tags":["services","un-static"]},{"location":"blog/2025/03/07/improved-shebang-for-your-python-standalone-script/","title":"Improved shebang for your python standalone script","text":"<p>Sometimes you just want a script, without having to specify a virtualenv or polluting your global python environment. Given PEP-0723 we can build scripts as we would normally, though now add a few comments to it to specify the dependencies.</p> <p>When we combine that with uv we can easily run the script without having to activate a virtualenv or specify the python interpreter. And it will just take care of it. Assume the script has the name <code>script.py</code> we would call it with <code>uv run -s script.py</code>.</p> <p>This is great on its own, it still is less ideal when you want to be able to call this script as part of your path. How can we fix this?</p> <p>Normally we would have a shebang in the python file pointing to a executable with a static path or using <code>env</code> to find the python interpreter.</p> <p>But wouldn't it be great if we could have a shebang that would take care of it and will have the <code>uv run</code> as part of it and you would even have to think about it anymore when trying to run the command from the terminal?</p>","tags":["uv","python","script"]},{"location":"blog/2025/03/07/improved-shebang-for-your-python-standalone-script/#the-fix","title":"The fix","text":"<p>Let's update the shebang and make it automatically use <code>uv run</code> to run the script. This way we can just call the script as if it was a normal executable.</p> <pre><code>#!/usr/bin/env -S uv run --script\n</code></pre> <p>The <code>-S</code> for <code>env</code> has the following in the man page: <pre><code>    -S string\n            Split apart the given string into multiple strings, and process each of the resulting strings as separate arguments\n            to the env utility.  The -S option recognizes some special character escape sequences and also supports\n            environment-variable substitution, as described below.\n</code></pre></p> <p>and the <code>--script</code> for <code>uv</code> has the following:</p> <pre><code>  -s, --script\n          Run the given path as a Python script.\n\n          Using `--script` will attempt to parse the path as a PEP 723 script, irrespective of its extension.\n</code></pre>","tags":["uv","python","script"]},{"location":"blog/2025/03/07/improved-shebang-for-your-python-standalone-script/#example","title":"Example","text":"<pre><code>#!/usr/bin/env -S uv run --script\n\n# /// script\n# requires-python = \"&gt;=3.12\"\n# dependencies = [\n#   \"click\",\n# ]\n# ///\n\nimport json\nfrom ipaddress import ip_address, ip_network\nfrom pathlib import Path\n\nimport click\n\n\n@click.command()\n@click.argument(\"peer_ip\")\ndef main(peer_ip: str) -&gt; None:\n    JSONFILE = Path(\"data.json\")\n    json_data = json.loads(JSONFILE.read_text())\n    ...\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["uv","python","script"]},{"location":"blog/2025/03/07/improved-shebang-for-your-python-standalone-script/#conclusion","title":"Conclusion","text":"<p>With this in place, we can just call the script from anywhere on the CLI and have <code>uv</code> take care of the dependencies and we will not have to provide the <code>uv run --script</code> ourselves.</p> <pre><code>/path/to/script.py\n</code></pre> <p>and its done.</p> <p>Share on </p>","tags":["uv","python","script"]},{"location":"contact/","title":"Contact","text":"Name Email Subject Message Send"},{"location":"contact/mailsent/","title":"Mail sent","text":"<p>Your message has been sent</p>"},{"location":"speaker/","title":"Index","text":""},{"location":"speaker/#public-speaking","title":"Public speaking","text":""},{"location":"speaker/#presentations","title":"Presentations","text":""},{"location":"speaker/#bmp-going-beyond-show-commands-and-screen-scraping","title":"BMP - Going beyond show commands and screen scraping","text":"<ul> <li>Date: 2025-05-30</li> <li>Location: Autocon3</li> <li>Links:<ul> <li>Video</li> </ul> </li> </ul>"},{"location":"speaker/#description","title":"Description","text":"<p> With the automation around configuration generation and deployment already in place. It was the desire to present back to the customer that the BGP prefixes configured to be advertised were arrived as such. Though not only that, but also accepted through the policy.</p> <p>I will take you on the journey from where some felt that screen scraping was a good idea, to the current solution using BMP (BGP Monitoring Protocol). The chosen architecture, the challenges faced with the amount of data that comes with BGP and how we dealt with them.</p> <p>If you were looking on how to deal with your BGP data, especially your pre policy data, this should kickstart your adventure.</p>"},{"location":"speaker/#repos-are-like-children-parenting-101-pygrunn-2025","title":"Repos are like children, parenting 101 - PyGrunn 2025","text":"<ul> <li>Date: 2025-05-16</li> <li>Location: PyGrunn</li> <li>Links: Video</li> </ul>"},{"location":"speaker/#description_1","title":"Description","text":"<p> For those without children, you might never have realized that having a project is much like a child. At the same, for those with children, you may never have the time or piece of mind to realize it.</p> <p>I\u2019ll take you through a walk of life on \u2018creating\u2019 your child(ren), how to deal with the early stages, and taking care of the rules and boundaries as these youngsters mature. How do you ensure that you raise a child to be proud of?</p> <p>What happens when your child is going through different phases in life? Going from kindergarten, to school and beyond. How will he behave? Will he be overwhelmed by all the new info/data thrown at him?</p> <p>And so, you become a proud parent and have the brilliant idea (or someone had an idea for you) of having another child. Do you leave it to fate that he will mature the same, or could you influence it in any way?</p> <p>Would it be possible to provide our children the tools to take care of themselves?</p> <p>For whoever got confused, we are still talking about repositories and code. ;)</p>"},{"location":"speaker/#convert-poetry-to-uv-lightning","title":"Convert Poetry to uv - lightning","text":"<ul> <li>Date: 2025-03-27</li> <li>Location: Py.Amsterdam</li> <li>Links: Meetup</li> </ul>"},{"location":"speaker/#description_2","title":"Description","text":"<p> During the python meetup there was room for some 5 minute lightning talks. I stepped up and shared my findings on poetry and uv, and the challenging with migrating from poetry to uv. Because of this challenge I've written a tool to help with that migration. Allowing to pass in the current pyproject.toml and convert it to uv.</p> <p>The tool is hosted at github - convert_poetry2uv.</p>"},{"location":"speaker/#the-tale-of-2-henrys-and-bmp","title":"The Tale of 2 Henrys and BMP","text":"<p>What building cars can teach us about building software and BMP</p> <ul> <li>Date: 2025-03-06</li> <li>Location: DKNOG15</li> <li>Links: Video, Slides</li> </ul>"},{"location":"speaker/#description_3","title":"Description","text":"<p> In the late 19th century, two industrial titans were born within a few months of each other but an ocean apart. Both of these men, Henry Royce and Henry Ford, were obsessed with precision engineering and, fortunately for many of us, cars. They focused on building the best motor cars possible, though they achieved their goals in very different ways: Royce was driven by perfection, Ford by production.</p> <p>Michael Daly (Senior Director of Engineering) and Bart Dorlandt (Senior Network Automation Engineer) are working at Imperva where they are undergoing a complete rewrite of the Automation platform and we are using some of the ideas that these engineers have taught us us.</p> <p>This presentation will explore our vision for building better, more sustainable tools and discuss how the Network Automation team at Imperva is implementing these principles in our workflow.</p> <p>From the technical side we zoom in on a recent project using BGP Monitoring Protocol (BMP). Where we previously had manual network verification, which improved to screen scraping, now having a push model from the router to a central database. This gives us an \u201coffline\u201d state and allows the customer to self verify their configured and advertised prefixes are accepted and learned as expected. This didn\u2019t happen without challenges. We will share the pitfalls and the challenges and how we faced and overcame them.</p>"},{"location":"speaker/#repos-are-like-children-pyutrecht-2024","title":"Repos are like children - PyUtrecht 2024","text":"<p>parenting 101</p> <ul> <li>Date: 2024-09-17</li> <li>Location: PyUtrecht</li> <li>Links: Video - starting at 34:00, Slides</li> </ul>"},{"location":"speaker/#description_4","title":"Description","text":"<p> For those without children, you might never have realized that having a project is much like a child. At the same, for those with children, you may never have the time or piece of mind to realize it.</p> <p>I\u2019ll take you through a walk of life on \u2018creating\u2019 your child(ren), how to deal with the early stages, and taking care of the rules and boundaries as these youngsters mature. How do you ensure that you raise a child to be proud of?</p> <p>What happens when your child is going through different phases in life? Going from kindergarten, to school and beyond. How will he behave? Will he be overwhelmed by all the new info/data thrown at him?</p> <p>And so, you become a proud parent and have the brilliant idea (or someone had an idea for you) of having another child. Do you leave it to fate that he will mature the same, or could you influence it in any way?</p> <p>Would it be possible to provide our children the tools to take care of themselves?</p> <p>For whoever got confused, we are still talking about repositories and code. ;)</p> <p>During the talk we reference the growth of the child to several aspects of growing your repository and code. We therefore touch on the following subjects:</p> <ul> <li>poetry, rye, uv</li> <li>ruff</li> <li>cookiecutter</li> <li>pytest</li> <li>pipelines</li> <li>coverage, though use it wisely</li> </ul>"},{"location":"speaker/#podcasts","title":"Podcasts","text":""},{"location":"speaker/#there-must-be-a-better-way-network-automation-nerds","title":"There Must Be a Better Way! - Network Automation Nerds","text":"<ul> <li>Date: 2025-06-04</li> <li>Podcast: Packet Pushers - Network Automation Nerds</li> <li>Podcast links: Apple podcast, Spotify, Overcast, PocketCasts</li> </ul>"},{"location":"speaker/#description_5","title":"Description","text":"<p> \u201cThere must be a better way!\u201d is guest Bart Dorlandt\u2019s motto, which he applies to network automation, among other things. In today\u2019s episode, Bart shares what he\u2019s learned about network automation, explains why he focuses on process over tools, and reflects on the importance of mentorship. Bart and Eric also discuss why even if listeners aren\u2019t working on big automation projects, they can still look for better ways to manage their networks.</p> <ul> <li>Links mentioned in the podcast:<ul> <li>Convert Poetry to UV</li> <li>Convert Poetry to UV - PyPI</li> <li>Beyond the Makefile</li> <li>Raymond Hettinger Youtube videos</li> <li>Task</li> </ul> </li> </ul>"},{"location":"speaker/#deploying-the-bgp-monitoring-protocol-bmp-at-isp-scale","title":"Deploying the BGP Monitoring Protocol (BMP) at ISP Scale","text":"<ul> <li>Date: 2024-11-13</li> <li>Podcast: Packet Pushers - Heavy Networking</li> <li>Podcast links: Apple podcast, Spotify, Overcast, PocketCasts</li> </ul>"},{"location":"speaker/#description_6","title":"Description","text":"<p>The BGP Monitoring Protocol, or BMP, is an IETF standard. With BMP you can send BGP prefixes and updates from a router to a collector before any policy filters are applied. Once collected, you can analyze this routing data without any impact on the router itself. On today\u2019s Heavy Networking, we talk with Bart Dorlandt, a network automation solutions architect. An ISP approached Bart with a use case for BMP, and he designed and built a solution to serve the ISP\u2019s customers.</p> <p>We discuss what BMP is good for, how it works, and why Bart needed to use BMP. We also get into the tools he used to build his solution, the architecture he designed, the challenges he ran into in dealing with millions of records, why Kafka was essential for scaling, and more.</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/ansible/","title":"ansible","text":""},{"location":"blog/category/taskfile/","title":"taskfile","text":""},{"location":"blog/category/python/","title":"python","text":""},{"location":"blog/category/paperless/","title":"paperless","text":""},{"location":"blog/category/services/","title":"Services","text":""}]}